{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoH6TbXR2-DJ",
        "outputId": "9ea5c71d-eab2-48ec-902f-ed1a16602dd3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset Shape: (15369, 131)\n",
            "üîç Missing Value Percentage (Non-Zero Only):\n",
            "School Program                                   100.000000\n",
            "Number of Enrollments                            100.000000\n",
            "Diversity Spotlight (US Headquarters Only)       100.000000\n",
            "School Method                                    100.000000\n",
            "Number of Private Contacts                       100.000000\n",
            "Tags                                             100.000000\n",
            "School Type                                      100.000000\n",
            "Number of Private Notes                          100.000000\n",
            "Number of Founders (Alumni)                       99.954454\n",
            "Accelerator Application Deadline                  99.928427\n",
            "Accelerator Duration (in weeks)                   99.915414\n",
            "Accelerator Program Type                          99.908908\n",
            "CB Rank (School)                                  99.895894\n",
            "Valuation at IPO Currency (in USD)                99.785282\n",
            "Valuation at IPO                                  99.785282\n",
            "Valuation at IPO Currency                         99.785282\n",
            "Delisted Date                                     99.557551\n",
            "Last Layoff Mention Date                          99.407899\n",
            "Money Raised at IPO                               99.368859\n",
            "Money Raised at IPO Currency                      99.368859\n",
            "Money Raised at IPO Currency (in USD)             99.368859\n",
            "Delisted Date Precision                           99.316807\n",
            "Number of Diversity Investments                   99.310300\n",
            "Actively Hiring                                   99.232221\n",
            "Hub Tags                                          99.199688\n",
            "Investment Stage                                  98.880864\n",
            "Number of Exits                                   98.457935\n",
            "Number of Exits (IPO)                             98.457935\n",
            "Acquisition Terms                                 98.431908\n",
            "Closed Date                                       98.210684\n",
            "Investor Type                                     98.100072\n",
            "Number of Lead Investments                        97.917887\n",
            "Price Currency                                    97.800768\n",
            "Price                                             97.800768\n",
            "Price Currency (in USD)                           97.800768\n",
            "Last Leadership Hiring Date                       97.286746\n",
            "Number of Portfolio Organizations                 96.727178\n",
            "Number of Investments                             96.727178\n",
            "Apptopia - Downloads Last 30 Days                 95.204633\n",
            "Number of Acquisitions                            92.621511\n",
            "Number of Events                                  92.283167\n",
            "Closed Date Precision                             91.632507\n",
            "IPqwery - Most Popular Patent Class               91.476348\n",
            "Acquisition Type                                  91.320190\n",
            "Stock Exchange                                    91.085952\n",
            "Stock Symbol                                      91.059926\n",
            "IPO Date                                          91.046913\n",
            "Stock Symbol URL                                  91.046913\n",
            "Acquired by                                       90.767129\n",
            "Acquired by URL                                   90.767129\n",
            "Announced Date Precision                          90.767129\n",
            "Transaction Name URL                              90.767129\n",
            "Transaction Name                                  90.767129\n",
            "Announced Date                                    90.767129\n",
            "Apptopia - Number of Apps                         84.761533\n",
            "Acquisition Status                                84.494762\n",
            "Exit Date                                         82.607847\n",
            "Exit Date Precision                               82.607847\n",
            "IPqwery - Most Popular Trademark Class            79.621316\n",
            "Aberdeen - IT Spend                               79.159347\n",
            "Aberdeen - IT Spend Currency (in USD)             79.159347\n",
            "Aberdeen - IT Spend Currency                      79.152840\n",
            "Number of Lead Investors                          77.519682\n",
            "SEMrush - Average Visits (6 months)               76.250895\n",
            "IPqwery - Trademarks Registered                   76.010150\n",
            "IPqwery - Patents Granted                         76.010150\n",
            "SEMrush - Visit Duration Growth                   72.275359\n",
            "Last Equity Funding Amount Currency (in USD)      71.963042\n",
            "Last Equity Funding Amount                        71.963042\n",
            "Last Equity Funding Amount Currency               71.943523\n",
            "SEMrush - Bounce Rate Growth                      69.464506\n",
            "Total Equity Funding Amount Currency (in USD)     69.321361\n",
            "Total Equity Funding Amount                       69.321361\n",
            "Last Funding Amount Currency (in USD)             67.649164\n",
            "Last Funding Amount                               67.649164\n",
            "Last Funding Amount Currency                      67.629644\n",
            "SEMrush - Monthly Rank Growth                     66.959464\n",
            "SEMrush - Page Views / Visit Growth               66.959464\n",
            "SEMrush - Monthly Rank Change (#)                 66.959464\n",
            "SEMrush - Monthly Visits Growth                   66.959464\n",
            "G2 Stack - Total Products Active                  65.918407\n",
            "Top 5 Investors                                   64.909884\n",
            "Number of Investors                               64.890364\n",
            "Total Funding Amount                              63.621576\n",
            "Total Funding Amount Currency (in USD)            63.621576\n",
            "Number of Articles                                61.318238\n",
            "Estimated Revenue Range                           60.992908\n",
            "Total Equity Funding Amount Currency              60.042944\n",
            "Last Equity Funding Type                          60.042944\n",
            "SEMrush - Visit Duration                          59.431323\n",
            "SEMrush - Global Traffic Rank                     59.431323\n",
            "SEMrush - Page Views / Visit                      59.431323\n",
            "SEMrush - Bounce Rate                             59.431323\n",
            "SEMrush - Monthly Visits                          59.431323\n",
            "Funding Status                                    58.286160\n",
            "Contact Job Departments                           54.557876\n",
            "Last Funding Date                                 53.705511\n",
            "Number of Funding Rounds                          53.705511\n",
            "Last Funding Type                                 53.705511\n",
            "Total Funding Amount Currency                     53.705511\n",
            "Headquarters Regions                              53.445247\n",
            "Number of Contacts                                51.441213\n",
            "Founders                                          48.682413\n",
            "Number of Founders                                48.675906\n",
            "Facebook                                          41.245364\n",
            "Twitter                                           38.428004\n",
            "Phone Number                                      36.469517\n",
            "Full Description                                  27.737654\n",
            "Contact Email                                     26.006897\n",
            "LinkedIn                                          23.274123\n",
            "BuiltWith - Active Tech Count                     17.021277\n",
            "Similar Companies                                 10.280435\n",
            "Number of Employees                                5.224803\n",
            "Industries                                         3.240289\n",
            "Industry Groups                                    3.240289\n",
            "Company Type                                       2.296831\n",
            "Website                                            1.197215\n",
            "Founded Date Precision                             0.208211\n",
            "Founded Date                                       0.208211\n",
            "CB Rank (Company)                                  0.026026\n",
            "Trend Score (90 Days)                              0.013013\n",
            "Trend Score (30 Days)                              0.013013\n",
            "CB Rank (Organization)                             0.013013\n",
            "Trend Score (7 Days)                               0.013013\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2974679807.py:5: DtypeWarning: Columns (101,121) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1.1: Load dataset\n",
        "file_path = '/content/Canada.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1.2: Show shape\n",
        "print(f\"‚úÖ Dataset Shape: {df.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 2.1: Select 31 core columns based on the hackathon PDF/docx guidance\n",
        "important_columns = [\n",
        "    \"Founded Date\", \"Number of Founders\", \"Company Type\", \"Number of Employees\",\n",
        "    \"Industries\", \"Headquarters Location\", \"Headquarters Regions\", \"Number of Investors\",\n",
        "    \"Actively Hiring\", \"Number of Funding Rounds\", \"Last Funding Amount\",\n",
        "    \"Funding Status\", \"Last Funding Type\", \"Estimated Revenue Range\", \"IPqwery - Patents Granted\",\n",
        "    \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"SEMrush - Page Views / Visit\",\n",
        "    \"SEMrush - Bounce Rate\", \"Number of Events\", \"BuiltWith - Active Tech Count\",\n",
        "    \"G2 Stack - Total Products Active\", \"Number of Articles\", \"CB Rank (Company)\",\n",
        "    \"Total Funding Amount\", \"Valuation at IPO\", \"Price\", \"Number of Exits\",\n",
        "    \"Industry Groups\", \"Apptopia - Downloads Last 30 Days\", \"Apptopia - Number of Apps\"\n",
        "]\n",
        "\n",
        "# Subset the dataframe\n",
        "df_cleaned = df[important_columns].copy()\n",
        "\n",
        "# Save to a new CSV working file\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Created working file: /content/Canada_cleaned.csv with 31 key columns\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0aHe90P3sUw",
        "outputId": "e4a88224-3403-4e50-fb57-bbb247f6e309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created working file: /content/Canada_cleaned.csv with 31 key columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.1: Fix CB Rank (Company) format\n",
        "df_cleaned[\"CB Rank (Company)\"] = df_cleaned[\"CB Rank (Company)\"].astype(str).str.replace(\",\", \"\")\n",
        "df_cleaned[\"CB Rank (Company)\"] = pd.to_numeric(df_cleaned[\"CB Rank (Company)\"], errors='coerce')\n",
        "\n",
        "# Now fill missing values\n",
        "print(\"Before:\", df_cleaned['CB Rank (Company)'].isnull().sum())\n",
        "\n",
        "# Grouped median by industry (if applicable)\n",
        "if 'Industries' in df_cleaned.columns:\n",
        "    df_cleaned['CB Rank (Company)'] = df_cleaned.groupby('Industries')[\"CB Rank (Company)\"].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "\n",
        "# Fallback to global median\n",
        "df_cleaned[\"CB Rank (Company)\"] = df_cleaned[\"CB Rank (Company)\"].fillna(df_cleaned[\"CB Rank (Company)\"].median())\n",
        "\n",
        "print(\"After:\", df_cleaned['CB Rank (Company)'].isnull().sum())\n",
        "\n",
        "# Save updated file\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ CB Rank (Company) cleaned and imputed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8U8E2zm4W98",
        "outputId": "6478b513-52ab-408e-c4c6-870984ffa691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 4\n",
            "After: 0\n",
            "‚úÖ CB Rank (Company) cleaned and imputed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'Founded Year' exists in the original dataset\n",
        "if 'Founded Year' in df.columns:\n",
        "    print(\"‚úÖ 'Founded Year' exists. Using it to fill missing 'Founded Date'\")\n",
        "\n",
        "    # Convert 'Founded Date' to datetime\n",
        "    df_cleaned['Founded Date'] = pd.to_datetime(df_cleaned['Founded Date'], errors='coerce')\n",
        "\n",
        "    # Use 'Founded Year' to fill missing 'Founded Date'\n",
        "    df_cleaned['Founded Date'] = df_cleaned['Founded Date'].fillna(\n",
        "        pd.to_datetime(df['Founded Year'], errors='coerce')\n",
        "    )\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è 'Founded Year' not found. Will impute using group-wise median\")\n",
        "\n",
        "    # Convert 'Founded Date' to datetime\n",
        "    df_cleaned['Founded Date'] = pd.to_datetime(df_cleaned['Founded Date'], errors='coerce')\n",
        "\n",
        "    # Grouped median fill by Industry\n",
        "    df_cleaned['Founded Date'] = df_cleaned.groupby('Industries')['Founded Date'].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "\n",
        "# Final fallback to global median\n",
        "df_cleaned['Founded Date'] = df_cleaned['Founded Date'].fillna(df_cleaned['Founded Date'].median())\n",
        "\n",
        "# Check result\n",
        "print(\"‚úÖ Founded Date Missing After Imputation:\", df_cleaned['Founded Date'].isnull().sum())\n",
        "\n",
        "# Save update\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Founded Date imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccoDMIiw40Na",
        "outputId": "3d0da544-53d1-47ed-a112-326ee752f922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è 'Founded Year' not found. Will impute using group-wise median\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
            "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Founded Date Missing After Imputation: 0\n",
            "‚úÖ Founded Date imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.3: Impute Company Type\n",
        "\n",
        "print(\"Before:\", df_cleaned[\"Company Type\"].isnull().sum())\n",
        "\n",
        "# Mode per industry group\n",
        "df_cleaned[\"Company Type\"] = df_cleaned.groupby(\"Industries\")[\"Company Type\"].transform(\n",
        "    lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x\n",
        ")\n",
        "\n",
        "# Fallback global mode\n",
        "global_mode = df_cleaned[\"Company Type\"].mode().iloc[0]\n",
        "df_cleaned[\"Company Type\"] = df_cleaned[\"Company Type\"].fillna(global_mode)\n",
        "\n",
        "print(\"After:\", df_cleaned[\"Company Type\"].isnull().sum())\n",
        "\n",
        "# Save changes\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Company Type imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CluXUcP95EzI",
        "outputId": "a156497c-26aa-4881-813d-5abd2c49b6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 353\n",
            "After: 0\n",
            "‚úÖ Company Type imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.4: Impute Industries\n",
        "print(\"Before:\", df_cleaned[\"Industries\"].isnull().sum())\n",
        "\n",
        "# Group-based mode (by Company Type)\n",
        "df_cleaned[\"Industries\"] = df_cleaned.groupby(\"Company Type\")[\"Industries\"].transform(\n",
        "    lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x\n",
        ")\n",
        "\n",
        "# Fallback to global mode if still missing\n",
        "global_mode = df_cleaned[\"Industries\"].mode().iloc[0]\n",
        "df_cleaned[\"Industries\"] = df_cleaned[\"Industries\"].fillna(global_mode)\n",
        "\n",
        "print(\"After:\", df_cleaned[\"Industries\"].isnull().sum())\n",
        "\n",
        "# Save update\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Industries imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgJBD1995MF9",
        "outputId": "ee8b43f7-f445-4a97-faf7-656f89e3a806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 498\n",
            "After: 0\n",
            "‚úÖ Industries imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step A: Clean \"Number of Employees\"\n",
        "def parse_employees(val):\n",
        "    try:\n",
        "        val = str(val)\n",
        "        if \"-\" in val:\n",
        "            a, b = val.split(\"-\")\n",
        "            return (int(a.replace(\",\", \"\")) + int(b.replace(\",\", \"\"))) // 2\n",
        "        elif val.isdigit():\n",
        "            return int(val)\n",
        "        else:\n",
        "            return np.nan\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_cleaned[\"Number of Employees\"] = df_cleaned[\"Number of Employees\"].apply(parse_employees)\n",
        "\n",
        "# Step B: ML Imputation\n",
        "features = [\n",
        "    \"Estimated Revenue Range\", \"SEMrush - Monthly Visits\",\n",
        "    \"CB Rank (Company)\", \"Industries\", \"Funding Status\"\n",
        "]\n",
        "\n",
        "train_df = df_cleaned[df_cleaned[\"Number of Employees\"].notnull()].copy()\n",
        "test_df = df_cleaned[df_cleaned[\"Number of Employees\"].isnull()].copy()\n",
        "\n",
        "# Encode categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_encoded = train_df[features].copy()\n",
        "test_encoded = test_df[features].copy()\n",
        "\n",
        "for col in train_encoded.columns:\n",
        "    if train_encoded[col].dtype == \"object\":\n",
        "        train_encoded[col] = train_encoded[col].astype(str)\n",
        "        test_encoded[col] = test_encoded[col].astype(str)\n",
        "\n",
        "encoder.fit(train_encoded)\n",
        "X_train = encoder.transform(train_encoded)\n",
        "X_test = encoder.transform(test_encoded)\n",
        "\n",
        "y_train = train_df[\"Number of Employees\"]\n",
        "\n",
        "# Train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict missing values\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Fill in predictions\n",
        "df_cleaned.loc[df_cleaned[\"Number of Employees\"].isnull(), \"Number of Employees\"] = y_pred.astype(int)\n",
        "\n",
        "# Save\n",
        "print(\"‚úÖ Missing After:\", df_cleaned[\"Number of Employees\"].isnull().sum())\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Number of Employees imputed with RandomForest and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-hcjrIn5T-D",
        "outputId": "378cdd9d-bc21-4e66-b703-70ed8b474073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing After: 0\n",
            "‚úÖ Number of Employees imputed with RandomForest and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean numeric columns with commas\n",
        "numeric_fix_cols = [\"Last Funding Amount\", \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\"]\n",
        "\n",
        "for col in numeric_fix_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].astype(str).str.replace(\",\", \"\")\n",
        "    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n"
      ],
      "metadata": {
        "id": "UhlIzyGY6vJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß™ Unique values in Estimated Revenue Range (non-null):\")\n",
        "print(df_cleaned[\"Estimated Revenue Range\"].dropna().unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWI6PYDJ63tJ",
        "outputId": "9fd4d965-2c8c-4500-a9f5-7b8db5cb3e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Unique values in Estimated Revenue Range (non-null):\n",
            "['$1M to $10M' 'Less than $1M' '$10M to $50M' '$50M to $100M'\n",
            " '$100M to $500M' '$1B to $10B' '$500M to $1B' '$10B+']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === Step 0: Clean any comma-based numeric columns ===\n",
        "numeric_fix_cols = [\"Last Funding Amount\", \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\"]\n",
        "\n",
        "for col in numeric_fix_cols:\n",
        "    df_cleaned[col] = df_cleaned[col].astype(str).str.replace(\",\", \"\")\n",
        "    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "\n",
        "# === Step 1: Define usable rows and clean unexpected labels ===\n",
        "target_col = \"Estimated Revenue Range\"\n",
        "\n",
        "# Define expected ordered labels\n",
        "ordered_labels = [\n",
        "    \"Less than $1M\", \"$1M to $10M\", \"$10M to $50M\",\n",
        "    \"$50M to $100M\", \"$100M to $500M\", \"$500M to $1B\", \"$1B+\"\n",
        "]\n",
        "\n",
        "# Map all large values into final \"catch-all\" label\n",
        "df_cleaned[target_col] = df_cleaned[target_col].replace({\n",
        "    \"$1B to $10B\": \"$1B+\",\n",
        "    \"$10B+\": \"$1B+\"\n",
        "}).str.strip()\n",
        "\n",
        "# Filter training and test sets\n",
        "train_df = df_cleaned[df_cleaned[target_col].isin(ordered_labels)].copy()\n",
        "test_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "# === Step 2: Label encode the target ===\n",
        "label_mapping = {label: idx for idx, label in enumerate(ordered_labels)}\n",
        "inv_label_map = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "train_df[target_col] = train_df[target_col].map(label_mapping)\n",
        "\n",
        "# === Step 3: Define features and encode them ===\n",
        "features = [\n",
        "    \"Number of Employees\", \"CB Rank (Company)\", \"Funding Status\",\n",
        "    \"Last Funding Amount\", \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "cat_features = [\"Funding Status\", \"Industries\"]\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "# Encode categorical features\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].astype(str)\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "encoder.fit(X_train[cat_features])\n",
        "X_train[cat_features] = encoder.transform(X_train[cat_features])\n",
        "X_test[cat_features] = encoder.transform(X_test[cat_features])\n",
        "\n",
        "# === Step 4: Train the classifier ===\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, train_df[target_col])\n",
        "\n",
        "# === Step 5: Predict and map back to original labels ===\n",
        "y_pred = clf.predict(X_test)\n",
        "y_pred_labels = [inv_label_map[int(p)] for p in y_pred]\n",
        "\n",
        "# Fill the predicted labels into df_cleaned\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred_labels\n",
        "\n",
        "# === Step 6: Save updated file ===\n",
        "print(\"‚úÖ Estimated Revenue Range missing after:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Estimated Revenue Range imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532U2aQ05dKA",
        "outputId": "1bbe08c7-e622-47da-c610-37c7df0151a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Estimated Revenue Range missing after: 0\n",
            "‚úÖ Estimated Revenue Range imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# === Step A: Clean commas and convert to float ===\n",
        "df_cleaned[\"Last Funding Amount\"] = df_cleaned[\"Last Funding Amount\"].astype(str).str.replace(\",\", \"\")\n",
        "df_cleaned[\"Last Funding Amount\"] = pd.to_numeric(df_cleaned[\"Last Funding Amount\"], errors='coerce')\n",
        "\n",
        "# === Step B: Prepare training and test sets ===\n",
        "target_col = \"Last Funding Amount\"\n",
        "\n",
        "# Split based on availability\n",
        "train_df = df_cleaned[df_cleaned[target_col].notnull()].copy()\n",
        "test_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "# Features to use\n",
        "features = [\n",
        "    \"Estimated Revenue Range\", \"Number of Employees\", \"CB Rank (Company)\",\n",
        "    \"Industries\", \"Funding Status\"\n",
        "]\n",
        "\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]\n",
        "\n",
        "# Encode categorical columns\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "# Convert all categorical to string\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].astype(str)\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "# Ordinal encoding for categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "encoder.fit(X_train[cat_features])\n",
        "X_train[cat_features] = encoder.transform(X_train[cat_features])\n",
        "X_test[cat_features] = encoder.transform(X_test[cat_features])\n",
        "\n",
        "# Target variable\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# === Step C: Train the model ===\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === Step D: Predict missing values ===\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Fill in predictions\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Save updated file\n",
        "print(\"‚úÖ Last Funding Amount missing after:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Last Funding Amount imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR4KkE6K6dXN",
        "outputId": "5ff23f1b-3f4f-484a-d891-204168f1f1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Last Funding Amount missing after: 0\n",
            "‚úÖ Last Funding Amount imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß™ Unique values in 'Funding Status' (non-null):\")\n",
        "print(df_cleaned[\"Funding Status\"].dropna().unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLTcF8ww7kF9",
        "outputId": "cc6272c9-3ed7-4389-94f1-ad2132d469a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Unique values in 'Funding Status' (non-null):\n",
            "['Early Stage Venture' 'Seed' 'M&A' 'IPO' 'Late Stage Venture'\n",
            " 'Private Equity']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# === Step A: Target column ===\n",
        "target_col = \"Funding Status\"\n",
        "\n",
        "# Train/test split\n",
        "train_df = df_cleaned[df_cleaned[target_col].notnull()].copy()\n",
        "test_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "# Features used (already cleaned/imputed)\n",
        "features = [\n",
        "    \"Estimated Revenue Range\", \"Last Funding Amount\", \"CB Rank (Company)\",\n",
        "    \"Number of Employees\", \"Industries\"\n",
        "]\n",
        "\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\"]\n",
        "\n",
        "# Convert categoricals to string\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].astype(str)\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "# Ordinal encode categoricals\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "encoder.fit(X_train[cat_features])\n",
        "X_train[cat_features] = encoder.transform(X_train[cat_features])\n",
        "X_test[cat_features] = encoder.transform(X_test[cat_features])\n",
        "\n",
        "# Prepare target\n",
        "y_train = train_df[target_col].astype(str)\n",
        "\n",
        "# === Step B: Train the model ===\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# === Step C: Predict and fill ===\n",
        "y_pred = clf.predict(X_test)\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Save file\n",
        "print(\"‚úÖ Funding Status missing after:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Funding Status imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY8rsVhh8HQl",
        "outputId": "97a1c316-be56-4021-b7d5-d13eb5cc7ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Funding Status missing after: 0\n",
            "‚úÖ Funding Status imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# === Step A: Clean column (remove commas if any) ===\n",
        "df_cleaned[\"Total Funding Amount\"] = df_cleaned[\"Total Funding Amount\"].astype(str).str.replace(\",\", \"\")\n",
        "df_cleaned[\"Total Funding Amount\"] = pd.to_numeric(df_cleaned[\"Total Funding Amount\"], errors='coerce')\n",
        "\n",
        "# === Step B: Train/test split ===\n",
        "target_col = \"Total Funding Amount\"\n",
        "train_df = df_cleaned[df_cleaned[target_col].notnull()].copy()\n",
        "test_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "# === Step C: Features for prediction ===\n",
        "features = [\n",
        "    \"Last Funding Amount\", \"Estimated Revenue Range\", \"Number of Employees\",\n",
        "    \"CB Rank (Company)\", \"Industries\", \"Funding Status\"\n",
        "]\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]\n",
        "\n",
        "# === Step D: Encode categorical features ===\n",
        "X_train = train_df[features].copy()\n",
        "X_test = test_df[features].copy()\n",
        "\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].astype(str)\n",
        "    X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "encoder.fit(X_train[cat_features])\n",
        "\n",
        "X_train[cat_features] = encoder.transform(X_train[cat_features])\n",
        "X_test[cat_features] = encoder.transform(X_test[cat_features])\n",
        "\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# === Step E: Train model ===\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# === Step F: Predict and fill ===\n",
        "y_pred = model.predict(X_test)\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# === Step G: Save file ===\n",
        "print(\"‚úÖ Total Funding Amount missing after:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(\"‚úÖ Total Funding Amount imputed and saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NCDxEUh8blx",
        "outputId": "eea39594-3f43-468f-be3c-d8a1fe2fdf53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total Funding Amount missing after: 0\n",
            "‚úÖ Total Funding Amount imputed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# === Step A: Clean Valuation column ===\n",
        "target_col = \"Valuation at IPO\"\n",
        "df_cleaned[target_col] = df_cleaned[target_col].astype(str).str.replace(\",\", \"\")\n",
        "df_cleaned[target_col] = pd.to_numeric(df_cleaned[target_col], errors='coerce')\n",
        "\n",
        "# === Step B: Focus only on IPO companies ===\n",
        "ipo_df = df_cleaned[df_cleaned[\"Funding Status\"] == \"IPO\"].copy()\n",
        "\n",
        "# Train/test split (within IPO rows only)\n",
        "train_df = ipo_df[ipo_df[target_col].notnull()].copy()\n",
        "test_df = ipo_df[ipo_df[target_col].isnull()].copy()\n",
        "\n",
        "# If very few rows exist with actual values, just skip\n",
        "if len(train_df) < 10:\n",
        "    print(\"‚ö†Ô∏è Not enough IPO rows with real valuations. Skipping imputation.\")\n",
        "else:\n",
        "    # === Step C: Define features ===\n",
        "    features = [\n",
        "        \"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\",\n",
        "        \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"\n",
        "    ]\n",
        "    cat_features = [\"Estimated Revenue Range\", \"Industries\"]\n",
        "\n",
        "    X_train = train_df[features].copy()\n",
        "    X_test = test_df[features].copy()\n",
        "\n",
        "    for col in cat_features:\n",
        "        X_train[col] = X_train[col].astype(str)\n",
        "        X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    encoder.fit(X_train[cat_features])\n",
        "    X_train[cat_features] = encoder.transform(X_train[cat_features])\n",
        "    X_test[cat_features] = encoder.transform(X_test[cat_features])\n",
        "\n",
        "    y_train = train_df[target_col]\n",
        "\n",
        "    # === Step D: Train model ===\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # === Step E: Predict and insert ===\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    df_cleaned.loc[test_df.index, target_col] = y_pred\n",
        "\n",
        "    # Save file\n",
        "    print(\"‚úÖ Valuation at IPO missing after:\", df_cleaned[target_col].isnull().sum())\n",
        "    df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "    print(\"‚úÖ Valuation at IPO imputed and saved (only for IPO-tagged startups).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPlY2-pTLdfV",
        "outputId": "77044c24-f7c5-48b2-af51-fba426d70ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Valuation at IPO missing after: 13375\n",
            "‚úÖ Valuation at IPO imputed and saved (only for IPO-tagged startups).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load cleaned file\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# Summary\n",
        "print(\"‚úÖ Shape:\", df_cleaned.shape)\n",
        "print(\"\\nüìâ Missing Values (%):\")\n",
        "print(df_cleaned.isnull().mean().sort_values(ascending=False) * 100)\n",
        "\n",
        "# Show sample rows\n",
        "df_cleaned.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y1OoUsx4Loni",
        "outputId": "44871140-3bbe-41f6-88fa-6f7b289b4d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Shape: (15369, 31)\n",
            "\n",
            "üìâ Missing Values (%):\n",
            "Actively Hiring                      99.232221\n",
            "Number of Exits                      98.457935\n",
            "Price                                97.800768\n",
            "Apptopia - Downloads Last 30 Days    95.204633\n",
            "Number of Events                     92.283167\n",
            "Valuation at IPO                     87.025831\n",
            "Apptopia - Number of Apps            84.761533\n",
            "IPqwery - Patents Granted            76.010150\n",
            "G2 Stack - Total Products Active     65.918407\n",
            "Number of Investors                  64.890364\n",
            "Number of Articles                   61.318238\n",
            "SEMrush - Bounce Rate                59.431323\n",
            "SEMrush - Page Views / Visit         59.431323\n",
            "SEMrush - Visit Duration             59.431323\n",
            "SEMrush - Monthly Visits             59.431323\n",
            "Last Funding Type                    53.705511\n",
            "Number of Funding Rounds             53.705511\n",
            "Headquarters Regions                 53.445247\n",
            "Number of Founders                   48.675906\n",
            "BuiltWith - Active Tech Count        17.021277\n",
            "Industry Groups                       3.240289\n",
            "Industries                            0.000000\n",
            "Headquarters Location                 0.000000\n",
            "Estimated Revenue Range               0.000000\n",
            "Funding Status                        0.000000\n",
            "Last Funding Amount                   0.000000\n",
            "Founded Date                          0.000000\n",
            "Company Type                          0.000000\n",
            "Number of Employees                   0.000000\n",
            "CB Rank (Company)                     0.000000\n",
            "Total Funding Amount                  0.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Founded Date  Number of Founders Company Type  Number of Employees  \\\n",
              "0   2014-01-01                 NaN   For Profit                175.0   \n",
              "1   2014-05-01                 NaN   For Profit                 30.0   \n",
              "2   2013-01-01                 2.0   For Profit                 75.0   \n",
              "3   2016-01-01                 NaN   For Profit                 75.0   \n",
              "4   2021-01-01                 NaN   For Profit                  5.0   \n",
              "\n",
              "                                          Industries  \\\n",
              "0                                           Software   \n",
              "1  Business Development, Digital Marketing, Profe...   \n",
              "2  Content Creators, Marketing, Social Media, Soc...   \n",
              "3          Advertising, Digital Marketing, Marketing   \n",
              "4                      Cloud Data Services, Internet   \n",
              "\n",
              "        Headquarters Location Headquarters Regions  Number of Investors  \\\n",
              "0    Vaughan, Ontario, Canada          Great Lakes                  NaN   \n",
              "1    Toronto, Ontario, Canada          Great Lakes                  NaN   \n",
              "2    Toronto, Ontario, Canada          Great Lakes                 22.0   \n",
              "3    Toronto, Ontario, Canada          Great Lakes                  NaN   \n",
              "4  Causapscal, Quebec, Canada                  NaN                  NaN   \n",
              "\n",
              "  Actively Hiring  Number of Funding Rounds  ...  \\\n",
              "0             NaN                       NaN  ...   \n",
              "1             NaN                       NaN  ...   \n",
              "2             NaN                       8.0  ...   \n",
              "3             NaN                       NaN  ...   \n",
              "4             NaN                       NaN  ...   \n",
              "\n",
              "   G2 Stack - Total Products Active Number of Articles CB Rank (Company)  \\\n",
              "0                               NaN                NaN          391288.5   \n",
              "1                               NaN                NaN         2477615.0   \n",
              "2                              37.0                  9           60890.0   \n",
              "3                               NaN                NaN          923626.0   \n",
              "4                               NaN                NaN         1914268.0   \n",
              "\n",
              "  Total Funding Amount Valuation at IPO  Price  Number of Exits  \\\n",
              "0         4.044198e+06              NaN    NaN              NaN   \n",
              "1         4.503167e+06              NaN    NaN              NaN   \n",
              "2         2.141862e+07              NaN    NaN              NaN   \n",
              "3         2.904902e+07              NaN    NaN              NaN   \n",
              "4         4.187781e+06              NaN    NaN              NaN   \n",
              "\n",
              "                                     Industry Groups  \\\n",
              "0                                                NaN   \n",
              "1  Other, Professional Services, Sales and Marketing   \n",
              "2  Internet Services, Media and Entertainment, Sa...   \n",
              "3                   Advertising, Sales and Marketing   \n",
              "4          Information Technology, Internet Services   \n",
              "\n",
              "  Apptopia - Downloads Last 30 Days  Apptopia - Number of Apps  \n",
              "0                               NaN                        NaN  \n",
              "1                               NaN                        NaN  \n",
              "2                               NaN                        NaN  \n",
              "3                               NaN                        NaN  \n",
              "4                               NaN                        NaN  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06bbd2e2-dd4e-464a-9f94-736ce51d2439\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Founded Date</th>\n",
              "      <th>Number of Founders</th>\n",
              "      <th>Company Type</th>\n",
              "      <th>Number of Employees</th>\n",
              "      <th>Industries</th>\n",
              "      <th>Headquarters Location</th>\n",
              "      <th>Headquarters Regions</th>\n",
              "      <th>Number of Investors</th>\n",
              "      <th>Actively Hiring</th>\n",
              "      <th>Number of Funding Rounds</th>\n",
              "      <th>...</th>\n",
              "      <th>G2 Stack - Total Products Active</th>\n",
              "      <th>Number of Articles</th>\n",
              "      <th>CB Rank (Company)</th>\n",
              "      <th>Total Funding Amount</th>\n",
              "      <th>Valuation at IPO</th>\n",
              "      <th>Price</th>\n",
              "      <th>Number of Exits</th>\n",
              "      <th>Industry Groups</th>\n",
              "      <th>Apptopia - Downloads Last 30 Days</th>\n",
              "      <th>Apptopia - Number of Apps</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014-01-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Profit</td>\n",
              "      <td>175.0</td>\n",
              "      <td>Software</td>\n",
              "      <td>Vaughan, Ontario, Canada</td>\n",
              "      <td>Great Lakes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>391288.5</td>\n",
              "      <td>4.044198e+06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014-05-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Profit</td>\n",
              "      <td>30.0</td>\n",
              "      <td>Business Development, Digital Marketing, Profe...</td>\n",
              "      <td>Toronto, Ontario, Canada</td>\n",
              "      <td>Great Lakes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2477615.0</td>\n",
              "      <td>4.503167e+06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Other, Professional Services, Sales and Marketing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>For Profit</td>\n",
              "      <td>75.0</td>\n",
              "      <td>Content Creators, Marketing, Social Media, Soc...</td>\n",
              "      <td>Toronto, Ontario, Canada</td>\n",
              "      <td>Great Lakes</td>\n",
              "      <td>22.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>37.0</td>\n",
              "      <td>9</td>\n",
              "      <td>60890.0</td>\n",
              "      <td>2.141862e+07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Internet Services, Media and Entertainment, Sa...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Profit</td>\n",
              "      <td>75.0</td>\n",
              "      <td>Advertising, Digital Marketing, Marketing</td>\n",
              "      <td>Toronto, Ontario, Canada</td>\n",
              "      <td>Great Lakes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>923626.0</td>\n",
              "      <td>2.904902e+07</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Advertising, Sales and Marketing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-01-01</td>\n",
              "      <td>NaN</td>\n",
              "      <td>For Profit</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Cloud Data Services, Internet</td>\n",
              "      <td>Causapscal, Quebec, Canada</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1914268.0</td>\n",
              "      <td>4.187781e+06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Information Technology, Internet Services</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06bbd2e2-dd4e-464a-9f94-736ce51d2439')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06bbd2e2-dd4e-464a-9f94-736ce51d2439 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06bbd2e2-dd4e-464a-9f94-736ce51d2439');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-272ee314-ff97-42e0-9ca3-25edf3d29ee6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-272ee314-ff97-42e0-9ca3-25edf3d29ee6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-272ee314-ff97-42e0-9ca3-25edf3d29ee6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cleaned"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = \"Industry Groups\"\n",
        "\n",
        "# A. Basic info\n",
        "print(f\"üîç Column: {col}\")\n",
        "print(f\"‚û°Ô∏è Missing: {df_cleaned[col].isnull().sum()} / {len(df_cleaned)}\")\n",
        "print(f\"‚û°Ô∏è Missing %: {df_cleaned[col].isnull().mean()*100:.2f}%\")\n",
        "\n",
        "# B. Unique non-null values\n",
        "print(\"\\nüß™ Unique non-null values:\")\n",
        "print(df_cleaned[col].dropna().unique()[:30])  # show first 30 unique values\n",
        "\n",
        "# C. Value counts (Top 10)\n",
        "print(\"\\nüìä Most common values:\")\n",
        "print(df_cleaned[col].value_counts().head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTYQ-ch9LokD",
        "outputId": "b34c6dc7-4b0f-44f3-c8b4-1936ed69844c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Column: Industry Groups\n",
            "‚û°Ô∏è Missing: 498 / 15369\n",
            "‚û°Ô∏è Missing %: 3.24%\n",
            "\n",
            "üß™ Unique non-null values:\n",
            "['Other, Professional Services, Sales and Marketing'\n",
            " 'Internet Services, Media and Entertainment, Sales and Marketing'\n",
            " 'Advertising, Sales and Marketing'\n",
            " 'Information Technology, Internet Services' 'Education'\n",
            " 'Clothing and Apparel, Design'\n",
            " 'Biotechnology, Content and Publishing, Media and Entertainment, Science and Engineering'\n",
            " 'Professional Services, Sales and Marketing'\n",
            " 'Information Technology, Internet Services, Professional Services, Software'\n",
            " 'Advertising, Design, Internet Services, Sales and Marketing, Software'\n",
            " 'Administrative Services, Professional Services'\n",
            " 'Administrative Services, Apps, Information Technology, Mobile, Privacy and Security, Software'\n",
            " 'Transportation' 'Professional Services' 'Education, Health Care, Sports'\n",
            " 'Advertising, Content and Publishing, Media and Entertainment, Sales and Marketing'\n",
            " 'Clothing and Apparel, Design, Information Technology, Internet Services, Media and Entertainment'\n",
            " 'Content and Publishing, Media and Entertainment, Music and Audio'\n",
            " 'Information Technology, Internet Services, Professional Services'\n",
            " 'Health Care'\n",
            " 'Artificial Intelligence, Data and Analytics, Education, Science and Engineering, Software'\n",
            " 'Artificial Intelligence, Data and Analytics, Science and Engineering, Software'\n",
            " 'Administrative Services, Education, Professional Services'\n",
            " 'Food and Beverage'\n",
            " 'Administrative Services, Internet Services, Professional Services'\n",
            " 'Media and Entertainment, Music and Audio, Video'\n",
            " 'Administrative Services, Design, Other, Real Estate, Science and Engineering'\n",
            " 'Commerce and Shopping, Travel and Tourism' 'Information Technology'\n",
            " 'Design, Information Technology, Internet Services, Professional Services, Software']\n",
            "\n",
            "üìä Most common values:\n",
            "Industry Groups\n",
            "Natural Resources                                      441\n",
            "Health Care                                            390\n",
            "Financial Services                                     300\n",
            "Biotechnology, Health Care, Science and Engineering    271\n",
            "Software                                               223\n",
            "Financial Services, Lending and Investments            183\n",
            "Real Estate                                            179\n",
            "Energy, Natural Resources                              157\n",
            "Energy, Natural Resources, Sustainability              131\n",
            "Information Technology, Software                       130\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Only fill nulls using mode per 'Industries'\n",
        "def fill_by_grouped_mode(df, target_col, group_col):\n",
        "    df[target_col] = df.groupby(group_col)[target_col].transform(\n",
        "        lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Apply\n",
        "df_cleaned = fill_by_grouped_mode(df_cleaned, \"Industry Groups\", \"Industries\")\n",
        "\n",
        "# Confirm\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[\"Industry Groups\"].isnull().sum())\n",
        "\n",
        "# Save\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcFGxlqiLohk",
        "outputId": "3361c713-c8d3-4eec-84cd-1eedd21a65d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Step A: Setup\n",
        "target_col = \"BuiltWith - Active Tech Count\"\n",
        "features = [\n",
        "    \"Number of Employees\", \"Total Funding Amount\",\n",
        "    \"Estimated Revenue Range\", \"Industry Groups\"\n",
        "]\n",
        "\n",
        "# Step B: Prepare a shared encoder for both train and test\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "\n",
        "# Combine for encoding\n",
        "non_null_rows = df_cleaned.dropna(subset=features)\n",
        "encoder.fit(non_null_rows[[\"Estimated Revenue Range\", \"Industry Groups\"]])\n",
        "\n",
        "# Encode both train and predict parts\n",
        "df_encoded = df_cleaned.copy()\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industry Groups\"]] = encoder.transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industry Groups\"]]\n",
        ")\n",
        "\n",
        "# Step C: Train the model\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step D: Predict and fill\n",
        "pred_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = pred_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Fill values into original dataframe\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Step E: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izuxM0XlLoe8",
        "outputId": "f8c18ba4-4b19-4ac2-88e6-a0253cb52af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = \"Number of Founders\"\n",
        "\n",
        "# A. Basic info\n",
        "print(f\"üîç Column: {col}\")\n",
        "print(f\"‚û°Ô∏è Missing: {df_cleaned[col].isnull().sum()} / {len(df_cleaned)}\")\n",
        "print(f\"‚û°Ô∏è Missing %: {df_cleaned[col].isnull().mean()*100:.2f}%\")\n",
        "\n",
        "# B. Unique values (non-null)\n",
        "print(\"\\nüß™ Unique non-null values:\")\n",
        "print(sorted(df_cleaned[col].dropna().unique()))\n",
        "\n",
        "# C. Correlation analysis (optional preview)\n",
        "print(\"\\nüìä Correlation with:\")\n",
        "print(df_cleaned[[col, \"Number of Employees\", \"Total Funding Amount\"]].corr())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsJaqc3DLoch",
        "outputId": "357d6204-ecca-471b-9531-fb9d16a882eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Column: Number of Founders\n",
            "‚û°Ô∏è Missing: 7481 / 15369\n",
            "‚û°Ô∏è Missing %: 48.68%\n",
            "\n",
            "üß™ Unique non-null values:\n",
            "[np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0), np.float64(14.0), np.float64(19.0), np.float64(20.0)]\n",
            "\n",
            "üìä Correlation with:\n",
            "                      Number of Founders  Number of Employees  \\\n",
            "Number of Founders              1.000000            -0.010644   \n",
            "Number of Employees            -0.010644             1.000000   \n",
            "Total Funding Amount            0.006965             0.451199   \n",
            "\n",
            "                      Total Funding Amount  \n",
            "Number of Founders                0.006965  \n",
            "Number of Employees               0.451199  \n",
            "Total Funding Amount              1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = \"Number of Founders\"\n",
        "group_cols = [\"Company Type\", \"Industries\"]\n",
        "\n",
        "# Step 1: Group-median logic\n",
        "df_cleaned[col] = df_cleaned.groupby(group_cols)[col].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "# Step 2: If any left, fill with global median\n",
        "df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
        "\n",
        "# Step 3: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syQ9L8dXLoaA",
        "outputId": "f9a3342f-f205-4322-deb9-76c2b84ba8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Step A: Define a function to extract region\n",
        "def extract_region(location):\n",
        "    if pd.isnull(location): return np.nan\n",
        "    parts = location.split(\",\")\n",
        "    if len(parts) == 3:\n",
        "        return parts[1].strip()  # Extract Province/State\n",
        "    return np.nan\n",
        "\n",
        "# Step B: Apply extraction\n",
        "df_cleaned[\"Extracted Region\"] = df_cleaned[\"Headquarters Location\"].apply(extract_region)\n",
        "\n",
        "# Step C: Fill missing values\n",
        "df_cleaned[\"Headquarters Regions\"] = df_cleaned[\"Headquarters Regions\"].fillna(df_cleaned[\"Extracted Region\"])\n",
        "\n",
        "# Step D: Still missing? Fill with 'Unknown'\n",
        "df_cleaned[\"Headquarters Regions\"] = df_cleaned[\"Headquarters Regions\"].fillna(\"Unknown\")\n",
        "\n",
        "# Step E: Drop helper column\n",
        "df_cleaned.drop(columns=[\"Extracted Region\"], inplace=True)\n",
        "\n",
        "# Step F: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[\"Headquarters Regions\"].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRBKVtizLoXS",
        "outputId": "6aa39e46-d13b-4992-8c73-654785400c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "target_col = \"Number of Funding Rounds\"\n",
        "features = [\n",
        "    \"Total Funding Amount\",\n",
        "    \"Last Funding Amount\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Funding Status\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "# Step A: Prepare Encoder\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded = df_cleaned.copy()\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Funding Status\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Funding Status\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Step B: Train model\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step C: Predict\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Step D: Impute\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Step E: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M95GLUt1O1lq",
        "outputId": "0dd9700e-ad51-43ce-e0e4-3357577b4f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned[\"Last Funding Type\"].dropna().unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lSl1gACPDO8",
        "outputId": "6db15bd2-d446-4c8c-f878-d5e7ed6c5098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Series B' 'Seed' 'Grant' 'Pre-Seed' 'Equity Crowdfunding'\n",
            " 'Non-equity Assistance' 'Venture - Series Unknown' 'Angel' 'Series C'\n",
            " 'Debt Financing' 'Private Equity' 'Series A' 'Undisclosed'\n",
            " 'Post-IPO Equity' 'Post-IPO Debt' 'Corporate Round'\n",
            " 'Product Crowdfunding' 'Series D' 'Convertible Note'\n",
            " 'Initial Coin Offering' 'Series E' 'Secondary Market'\n",
            " 'Post-IPO Secondary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "target_col = \"Last Funding Type\"\n",
        "features = [\n",
        "    \"Funding Status\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Total Funding Amount\",\n",
        "    \"Number of Funding Rounds\",\n",
        "    \"Industries\",\n",
        "    \"Last Funding Amount\"\n",
        "]\n",
        "\n",
        "# Step A: Ordinal Encode categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded = df_cleaned.copy()\n",
        "df_encoded[[\"Funding Status\", \"Estimated Revenue Range\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Funding Status\", \"Estimated Revenue Range\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Step B: Label encode the target\n",
        "label_mapping = {label: idx for idx, label in enumerate(df_encoded[target_col].dropna().unique())}\n",
        "inv_label_mapping = {v: k for k, v in label_mapping.items()}\n",
        "df_encoded[target_col] = df_encoded[target_col].map(label_mapping)\n",
        "\n",
        "# Step C: Prepare training and prediction sets\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict for missing\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = clf.predict(X_pred)\n",
        "\n",
        "# Step D: Impute predicted values\n",
        "predicted_labels = [inv_label_mapping[int(label)] for label in y_pred]\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = predicted_labels\n",
        "\n",
        "# Step E: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wqAOR5jPMMS",
        "outputId": "8133d9e0-3651-4d6f-e64e-57936a8a320f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "target_col = \"SEMrush - Monthly Visits\"\n",
        "features = [\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Number of Employees\",\n",
        "    \"Industries\",\n",
        "    \"Last Funding Amount\"\n",
        "]\n",
        "\n",
        "# Encode categorical features\n",
        "df_encoded = df_cleaned.copy()\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Drop rows without any of the features\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# Train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict for missing\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Impute values\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Save and confirm\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrmtPinfPX2y",
        "outputId": "3afb50ea-9d7f-4b44-8d5e-37625bb14026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target column\n",
        "target_col = \"SEMrush - Page Views / Visit\"\n",
        "features = [\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Number of Employees\",\n",
        "    \"SEMrush - Monthly Visits\"\n",
        "]\n",
        "\n",
        "# Encode categorical columns\n",
        "df_encoded = df_cleaned.copy()\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Drop rows with missing feature or target\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# Train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict missing\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Fill missing\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Save and confirm\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h765NcyAPlsH",
        "outputId": "f56dd8d4-7e39-4015-b202-87f494a4d5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target\n",
        "target_col = \"SEMrush - Visit Duration\"\n",
        "features = [\n",
        "    \"SEMrush - Page Views / Visit\",\n",
        "    \"SEMrush - Monthly Visits\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "# Ordinal encode\n",
        "df_encoded = df_cleaned.copy()\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Training data\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict missing\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Impute\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VULdjwTLQFA_",
        "outputId": "dd54a7a0-2d08-4ec3-a7c8-24df51f17821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target column\n",
        "target_col = \"SEMrush - Bounce Rate\"\n",
        "\n",
        "# Step A: Preprocess percentage column\n",
        "df_cleaned[target_col] = df_cleaned[target_col].apply(\n",
        "    lambda x: float(str(x).replace('%', '')) if pd.notnull(x) else np.nan\n",
        ")\n",
        "\n",
        "# Features for prediction\n",
        "features = [\n",
        "    \"SEMrush - Visit Duration\",\n",
        "    \"SEMrush - Page Views / Visit\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "# Encode categoricals\n",
        "df_encoded = df_cleaned.copy()\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industries\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industries\"]]\n",
        ")\n",
        "\n",
        "# Train data\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# Model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Fill\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Optional: Reconvert to percentage string if needed\n",
        "# df_cleaned[target_col] = df_cleaned[target_col].apply(lambda x: f\"{x:.2f}%\" if pd.notnull(x) else x)\n",
        "\n",
        "# Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbvzG5GbQSVa",
        "outputId": "73df7289-7477-46ff-91b2-2dc053dd0d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([col for col in df_cleaned.columns if \"SEMrush\" in col])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBrEcmNaQ7C2",
        "outputId": "cfaac061-1e9f-4891-bcf8-1c78e2c5252b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SEMrush - Monthly Visits', 'SEMrush - Visit Duration', 'SEMrush - Page Views / Visit', 'SEMrush - Bounce Rate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Target column\n",
        "target_col = \"Apptopia - Downloads Last 30 Days\"\n",
        "\n",
        "# Features\n",
        "features = [\n",
        "    \"Apptopia - Number of Apps\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Number of Employees\",\n",
        "    \"Funding Status\"\n",
        "]\n",
        "\n",
        "# Step 1: Encode categoricals\n",
        "df_encoded = df_cleaned.copy()\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "df_encoded[[\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]] = encoder.fit_transform(\n",
        "    df_encoded[[\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]]\n",
        ")\n",
        "\n",
        "# Step 2: Clean target (remove commas, convert to float)\n",
        "df_encoded[target_col] = df_encoded[target_col].astype(str).str.replace(\",\", \"\")\n",
        "df_encoded[target_col] = pd.to_numeric(df_encoded[target_col], errors=\"coerce\")\n",
        "\n",
        "# Step 3: Filter training data\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "\n",
        "# Step 4: Train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and fill missing values\n",
        "predict_df = df_encoded[df_cleaned[target_col].isnull()]\n",
        "X_pred = predict_df[features]\n",
        "y_pred = model.predict(X_pred)\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# Step 6: Save\n",
        "print(\"‚úÖ Missing after imputation:\", df_cleaned[target_col].isnull().sum())\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYnU0AV6RA14",
        "outputId": "2b70fc10-bb88-472e-de4b-9c7cb8fc189a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_cleaned[\"Apptopia - Number of Apps\"].dropna().unique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByG4_U4IRZZy",
        "outputId": "d1461024-8751-4ef5-925c-1b59abacd2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1.  16.   2.   7.   3.   4.  15.   5.   6.  32.   8.  19. 418.   9.\n",
            "  10.  83.  11.  43.  20.  14.  12.  27.  26.  39.  76.  78.  30.  22.\n",
            " 116.  56. 291. 268.  41.  42.  74.  17.  18.  21. 298.  52.  28.  13.\n",
            "  45.  81.  40.  92.  29.  53.  33.  35.  23.  65.  46.  25.  66. 153.\n",
            "  55.  38.  61.  50.  31. 235.   0.  36.  59. 191. 251.  90. 180. 198.\n",
            " 129. 100.  24. 231. 241.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# üìÇ Copy original cleaned dataset\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# üéØ Target column to impute\n",
        "target_col = \"Apptopia - Number of Apps\"\n",
        "\n",
        "# üîç Features to use\n",
        "features = [\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Funding Status\",\n",
        "    \"Apptopia - Downloads Last 30 Days\",\n",
        "    \"Number of Employees\"\n",
        "]\n",
        "\n",
        "# üßº Convert target column to numeric (remove commas, convert to float)\n",
        "df_encoded[target_col] = pd.to_numeric(df_encoded[target_col].astype(str).str.replace(\",\", \"\"), errors='coerce')\n",
        "\n",
        "# üßº Also clean any numeric-looking features if needed\n",
        "df_encoded[\"Apptopia - Downloads Last 30 Days\"] = pd.to_numeric(\n",
        "    df_encoded[\"Apptopia - Downloads Last 30 Days\"].astype(str).str.replace(\",\", \"\"), errors='coerce'\n",
        ")\n",
        "\n",
        "# üõë Filter rows for training\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# üß† Categorical features\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]\n",
        "\n",
        "# üîß Fill missing and ensure strings\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna(\"Unknown\")\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "# üß† Encode categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# üéØ Prepare training and prediction sets\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col].astype(float)\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# ü§ñ Train Random Forest Regressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# üîÆ Predict missing values\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# üíæ Fill back into main DataFrame\n",
        "df_encoded.loc[df_encoded[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# ‚úÖ Update cleaned dataframe\n",
        "df_cleaned[target_col] = df_encoded[target_col]\n",
        "\n",
        "# üíæ Save to CSV\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Final confirmation\n",
        "print(f\"‚úÖ Missing after imputation: {df_cleaned[target_col].isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILGtAdWSRwOC",
        "outputId": "40ec89f3-9d7e-4387-b4a1-511ae90608e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# üõ†Ô∏è Configuration\n",
        "target_col = \"SEMrush - Monthly Visits\"   # change this to your target column\n",
        "cat_features = [\"Industry Groups\", \"Estimated Revenue Range\", \"Funding Status\"]  # adjust as needed\n",
        "\n",
        "# üîç Filter rows for training (non-null target)\n",
        "train_df = df_cleaned[df_cleaned[target_col].notnull()].copy()\n",
        "predict_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "# üßº Clean target column (remove %, commas, convert to float)\n",
        "def clean_numeric_col(col):\n",
        "    return (\n",
        "        col.astype(str)\n",
        "           .str.replace(\",\", \"\", regex=False)\n",
        "           .str.replace(\"%\", \"\", regex=False)\n",
        "           .replace(\"nan\", pd.NA)\n",
        "           .astype(float)\n",
        "    )\n",
        "\n",
        "train_df[target_col] = clean_numeric_col(train_df[target_col])\n",
        "df_cleaned[target_col] = clean_numeric_col(df_cleaned[target_col])\n",
        "\n",
        "# üîß Fill missing and convert cat features to string for encoding\n",
        "for col in cat_features:\n",
        "    train_df[col] = train_df[col].astype(str).fillna(\"Unknown\")\n",
        "    predict_df[col] = predict_df[col].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "# üéØ Define features\n",
        "features = cat_features\n",
        "\n",
        "# üî¢ Encode categorical features safely\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "\n",
        "if not predict_df.empty:\n",
        "    predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# ‚úÖ Model training\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# üîÆ Prediction and imputation\n",
        "if not predict_df.empty:\n",
        "    X_pred = predict_df[features]\n",
        "    y_pred = model.predict(X_pred)\n",
        "    df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# ‚úÖ Final check\n",
        "print(f\"‚úÖ Missing after imputation: {df_cleaned[target_col].isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJsxvT8JR4Lj",
        "outputId": "e234d0c8-ff6e-4284-9215-bc6a7b33fbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üßº Check missing values count and percentage\n",
        "missing_info = (\n",
        "    df_cleaned.isnull().sum()\n",
        "    .to_frame(\"Missing Count\")\n",
        "    .assign(\n",
        "        Missing_Percentage=lambda x: (x[\"Missing Count\"] / len(df_cleaned)) * 100\n",
        "    )\n",
        "    .sort_values(by=\"Missing Count\", ascending=False)\n",
        ")\n",
        "\n",
        "# üñ®Ô∏è Show only columns with missing values\n",
        "missing_info = missing_info[missing_info[\"Missing Count\"] > 0]\n",
        "print(missing_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZIO7eQMSXcS",
        "outputId": "f3ffc526-3587-4612-c1d3-3cedd90b4e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  Missing Count  Missing_Percentage\n",
            "Actively Hiring                           15251           99.232221\n",
            "Number of Exits                           15132           98.457935\n",
            "Price                                     15031           97.800768\n",
            "Number of Events                          14183           92.283167\n",
            "Valuation at IPO                          13375           87.025831\n",
            "IPqwery - Patents Granted                 11682           76.010150\n",
            "G2 Stack - Total Products Active          10131           65.918407\n",
            "Number of Investors                        9973           64.890364\n",
            "Number of Articles                         9424           61.318238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# üßπ Copy DataFrame to avoid modifying original\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# üéØ Target column\n",
        "target_col = \"Number of Articles\"\n",
        "\n",
        "# üß† Features to predict the target\n",
        "features = [\n",
        "    \"Number of Employees\",\n",
        "    \"Total Funding Amount\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Funding Status\",\n",
        "    \"Number of Founders\"\n",
        "]\n",
        "\n",
        "# üßπ Clean target column: remove commas and convert to float\n",
        "df_encoded[target_col] = df_encoded[target_col].astype(str).str.replace(\",\", \"\").replace(\"nan\", pd.NA)\n",
        "df_encoded[target_col] = pd.to_numeric(df_encoded[target_col], errors='coerce')\n",
        "\n",
        "# üéØ Drop rows with missing values in features or target (for training)\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# üß† Categorical features\n",
        "cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "# Step 1: Fill NaNs with 'Unknown' and convert to string for encoding\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "# Step 2: Ordinal encoding\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# Step 3: Model training and prediction\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# Step 4: Fill predicted values\n",
        "df_encoded.loc[df_encoded[target_col].isnull(), target_col] = y_pred\n",
        "\n",
        "# ‚úÖ Update the main cleaned DataFrame\n",
        "df_cleaned[target_col] = df_encoded[target_col]\n",
        "\n",
        "# üíæ Save to CSV\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Final check\n",
        "print(f\"‚úÖ Missing after imputation: {df_cleaned[target_col].isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0tNIZiATeMK",
        "outputId": "f25e52f4-d9c8-4754-dce4-c53269d29698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Define target and features (REMOVE the missing column)\n",
        "target_col = \"Number of Investors\"\n",
        "features = [\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Funding Status\",\n",
        "    \"Total Funding Amount\",\n",
        "    \"Number of Funding Rounds\",\n",
        "    \"Last Funding Amount\"\n",
        "]\n",
        "\n",
        "# ‚úÖ Work on a fresh copy\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# ‚úÖ Handle numeric strings: remove commas and convert to float\n",
        "for col in [\"Total Funding Amount\", \"Last Funding Amount\"]:\n",
        "    df_encoded[col] = (\n",
        "        df_encoded[col]\n",
        "        .astype(str)\n",
        "        .str.replace(\",\", \"\", regex=False)\n",
        "        .replace(\"Unknown\", np.nan)\n",
        "        .astype(float)\n",
        "    )\n",
        "\n",
        "# ‚úÖ Split training and prediction sets\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# ‚úÖ Identify and encode categorical features\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\"]\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna(\"Unknown\")\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "# ‚úÖ Encode categorical columns\n",
        "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# ‚úÖ Prepare data for model\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col].astype(float)\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# ‚úÖ Train model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Predict\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# ‚úÖ Fill predictions back\n",
        "df_encoded.loc[df_encoded[target_col].isnull(), target_col] = y_pred\n",
        "df_cleaned[target_col] = df_encoded[target_col]\n",
        "\n",
        "# ‚úÖ Save file\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n",
        "\n",
        "# ‚úÖ Summary\n",
        "missing_after = df_cleaned[target_col].isnull().sum()\n",
        "print(f\"‚úÖ Missing after imputation: {missing_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94VRfM1RUCvJ",
        "outputId": "0d84f46a-e9db-4165-8c4c-60a4125aca68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# üßπ Step 1: Setup\n",
        "df_encoded = df_cleaned.copy()\n",
        "target_col = \"G2 Stack - Total Products Active\"\n",
        "\n",
        "# üß† Step 2: Select features (add/remove based on your domain knowledge)\n",
        "features = [\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\",\n",
        "    \"Funding Status\",\n",
        "    \"Number of Employees\",\n",
        "    \"Total Funding Amount\",\n",
        "    \"Apptopia - Number of Apps\",\n",
        "    \"Apptopia - Downloads Last 30 Days\"\n",
        "]\n",
        "\n",
        "# üß™ Step 3: Drop rows with missing feature or target for training\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# üßº Step 4: Identify categorical features to encode\n",
        "cat_features = train_df[features].select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# üí° Step 5: Preprocess categorical values\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "\n",
        "# üöÄ Step 6: Encode categorical columns\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "\n",
        "# üß† Step 7: Prepare train/predict sets\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col].astype(str).str.replace(\",\", \"\").astype(float)\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# üèóÔ∏è Step 8: Train model and predict\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# üíæ Step 9: Fill predicted values back into original dataframe\n",
        "df_encoded.loc[df_encoded[target_col].isnull(), target_col] = y_pred\n",
        "df_cleaned[target_col] = df_encoded[target_col]\n",
        "\n",
        "# ‚úÖ Step 10: Confirm\n",
        "print(f\"‚úÖ Missing after imputation: {df_cleaned[target_col].isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv8bCqWqVWg9",
        "outputId": "c5440e4a-07ac-4e0d-b9f4-0f9435fd0ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# --- Setup for Imputation ---\n",
        "target_col = \"Actively Hiring\"\n",
        "features = [\n",
        "    \"Number of Employees\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Funding Status\",\n",
        "    \"Company Type\"\n",
        "]\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# --- Preprocessing and Encoding ---\n",
        "# Identify categorical features\n",
        "cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "# Prepare training data: remove rows where target is missing\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# Handle 'nan' values in categorical features for encoding\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "# Use OrdinalEncoder to transform categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# --- Train and Predict ---\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# Train a RandomForestClassifier model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the missing values\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# --- Impute and Save ---\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsUo6pmIWlg4",
        "outputId": "0d7ed732-84b9-44ce-d370-78da6ad0b1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation for Actively Hiring: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# --- Setup for Imputation ---\n",
        "target_col = \"Number of Exits\"\n",
        "\n",
        "features = [\n",
        "    \"Total Funding Amount\",\n",
        "    \"Number of Employees\",\n",
        "    \"Funding Status\",\n",
        "    \"Industries\",\n",
        "    \"Company Type\"\n",
        "]\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# --- Preprocessing and Encoding ---\n",
        "# Ensure numeric features are floats\n",
        "df_encoded['Total Funding Amount'] = pd.to_numeric(df_encoded['Total Funding Amount'], errors='coerce')\n",
        "df_encoded['Number of Employees'] = pd.to_numeric(df_encoded['Number of Employees'], errors='coerce')\n",
        "\n",
        "# Identify categorical features\n",
        "cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "# Prepare training data: remove rows where target is missing\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# Handle 'nan' values in categorical features for encoding\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "# Use OrdinalEncoder to transform categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# --- Train and Predict ---\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# Train a RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the missing values\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# --- Impute and Save ---\n",
        "# Fill with predicted values, rounded to the nearest integer\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = np.round(y_pred)\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD-fkJtAYZi8",
        "outputId": "648db99a-2b49-41a6-f2e3-27824c3bad4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation for Number of Exits: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# --- Setup for Imputation ---\n",
        "target_col = \"Price\"\n",
        "features = [\n",
        "    \"Total Funding Amount\",\n",
        "    \"Valuation at IPO\",\n",
        "    \"Number of Employees\",\n",
        "    \"Funding Status\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# Filter for rows where a Price might exist (e.g., acquisitions, IPOs)\n",
        "price_relevant_df = df_encoded[df_encoded['Funding Status'].isin(['M&A', 'IPO', 'Private Equity'])].copy()\n",
        "\n",
        "# --- Preprocessing and Encoding ---\n",
        "# Ensure numeric features are floats\n",
        "for col in [\"Total Funding Amount\", \"Valuation at IPO\", \"Number of Employees\"]:\n",
        "    price_relevant_df[col] = pd.to_numeric(price_relevant_df[col], errors='coerce')\n",
        "\n",
        "# Identify categorical features\n",
        "cat_features = [col for col in features if price_relevant_df[col].dtype == 'object']\n",
        "\n",
        "# Split into training and prediction sets within the filtered data\n",
        "train_df = price_relevant_df.dropna(subset=features + [target_col])\n",
        "predict_df = price_relevant_df[price_relevant_df[target_col].isnull()]\n",
        "\n",
        "if len(train_df) < 5:\n",
        "    print(f\"‚ö†Ô∏è Not enough training data for {target_col}. Imputation skipped.\")\n",
        "else:\n",
        "    # Handle 'nan' values in categorical features for encoding\n",
        "    for col in cat_features:\n",
        "        train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "        predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "    # Use OrdinalEncoder to transform categorical features\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "    predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "    # --- Train and Predict ---\n",
        "    X_train = train_df[features]\n",
        "    y_train = train_df[target_col]\n",
        "    X_pred = predict_df[features]\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_pred)\n",
        "\n",
        "    # --- Impute and Save ---\n",
        "    df_cleaned.loc[predict_df.index, target_col] = y_pred\n",
        "\n",
        "# Save the final cleaned dataframe\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzAsJpdUYZfR",
        "outputId": "81af04f7-7d77-4ff3-cfea-f12fed71b4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Not enough training data for Price. Imputation skipped.\n",
            "‚úÖ Missing after imputation for Price: 15031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# --- Setup for Imputation ---\n",
        "target_col = \"Number of Events\"\n",
        "features = [\n",
        "    \"Number of Employees\",\n",
        "    \"Number of Articles\",\n",
        "    \"Number of Funding Rounds\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = df_cleaned.copy()\n",
        "\n",
        "# --- Preprocessing and Encoding ---\n",
        "# Ensure numeric features are floats\n",
        "for col in [\"Number of Employees\", \"Number of Articles\", \"Number of Funding Rounds\"]:\n",
        "    df_encoded[col] = pd.to_numeric(df_encoded[col], errors='coerce')\n",
        "\n",
        "# Identify categorical features\n",
        "cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "# Prepare training data: remove rows where target is missing\n",
        "train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "# Handle 'nan' values in categorical features for encoding\n",
        "for col in cat_features:\n",
        "    train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "    predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "# Use OrdinalEncoder to transform categorical features\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "# --- Train and Predict ---\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[target_col]\n",
        "X_pred = predict_df[features]\n",
        "\n",
        "# Train a RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the missing values\n",
        "y_pred = model.predict(X_pred)\n",
        "\n",
        "# --- Impute and Save ---\n",
        "df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = np.round(y_pred)\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "\n",
        "print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xY6Pr5xYZc0",
        "outputId": "e910f589-3c97-4051-d6ed-cc7002fc4815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Missing after imputation for Number of Events: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# --- Step A: Clean Valuation column ---\n",
        "target_col = \"Valuation at IPO\"\n",
        "df_cleaned[target_col] = pd.to_numeric(\n",
        "    df_cleaned[target_col].astype(str).str.replace(\",\", \"\"), errors='coerce'\n",
        ")\n",
        "\n",
        "# --- Step B: Focus only on IPO companies ---\n",
        "ipo_df = df_cleaned[df_cleaned[\"Funding Status\"] == \"IPO\"].copy()\n",
        "\n",
        "# --- Step C: Prepare Data and Features ---\n",
        "features = [\n",
        "    \"Total Funding Amount\",\n",
        "    \"Last Funding Amount\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"CB Rank (Company)\",\n",
        "    \"Number of Employees\",\n",
        "    \"Industries\"\n",
        "]\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\"]\n",
        "\n",
        "train_df = ipo_df.dropna(subset=features + [target_col])\n",
        "test_df = ipo_df[ipo_df[target_col].isnull()]\n",
        "\n",
        "if len(train_df) < 5:\n",
        "    print(\"‚ö†Ô∏è Not enough IPO rows with real valuations. Skipping imputation.\")\n",
        "elif test_df.empty:\n",
        "    print(f\"‚ÑπÔ∏è No missing values found for {target_col}. Nothing to predict.\")\n",
        "else:\n",
        "    # Handle NaNs in categorical features\n",
        "    for col in cat_features:\n",
        "        train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "        test_df.loc[:, col] = test_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "    test_df.loc[:, cat_features] = encoder.transform(test_df[cat_features])\n",
        "\n",
        "    X_train = train_df[features]\n",
        "    y_train = train_df[target_col]\n",
        "    X_test = test_df[features]\n",
        "\n",
        "    # --- Step D: Train model ---\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # --- Step E: Predict and insert ---\n",
        "    y_pred = model.predict(X_test)\n",
        "    df_cleaned.loc[test_df.index, target_col] = y_pred\n",
        "    print(f\"‚úÖ Imputed {len(test_df)} rows for {target_col}\")\n",
        "\n",
        "# Save file\n",
        "df_cleaned.to_csv('/content/Canada_cleaned.csv', index=False)\n",
        "print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbi-X1PxYZac",
        "outputId": "b5bf106e-469d-4335-91dd-43cf785459c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è No missing values found for Valuation at IPO. Nothing to predict.\n",
            "‚úÖ Missing after imputation for Valuation at IPO: 13375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_check = [\n",
        "    \"Price\",\n",
        "    \"Valuation at IPO\",\n",
        "    \"IPqwery - Patents Granted\",\n",
        "    \"G2 Stack - Total Products Active\"\n",
        "]\n",
        "\n",
        "for col in cols_to_check:\n",
        "    print(f\"\\n--- {col} ---\")\n",
        "    print(\"Unique count:\", df_cleaned[col].nunique(dropna=False))\n",
        "    print(\"Sample values:\")\n",
        "    print(df_cleaned[col].dropna().unique()[:20])  # first 20 unique non-null values\n",
        "\n"
      ],
      "metadata": {
        "id": "r82xHUS3Y66w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2c1380-6244-4b23-ff67-ef5d62221b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Price ---\n",
            "Unique count: 247\n",
            "Sample values:\n",
            "[5.0000000e+07 1.7000000e+07 1.1000000e+08 1.0500000e+08 1.1000000e+09\n",
            " 1.4884039e+07 4.7500000e+06 2.6600000e+08 9.0000000e+06 1.8000000e+06\n",
            " 4.0000000e+08 1.2000000e+07 2.7000000e+06 2.2500000e+08 5.0000000e+06\n",
            " 4.7000000e+07 2.1200000e+06 3.6000000e+07 1.1500000e+08 7.5000000e+07]\n",
            "\n",
            "--- Valuation at IPO ---\n",
            "Unique count: 1348\n",
            "Sample values:\n",
            "[7.74767914e+07 7.74855614e+07 3.36503026e+08 3.00833658e+08\n",
            " 9.94283827e+07 1.83602552e+08 1.45835925e+08 1.20607788e+08\n",
            " 7.21985942e+07 9.53598513e+07 6.36624351e+08 9.75453778e+07\n",
            " 1.21036488e+08 5.52377639e+08 1.45626652e+08 1.28890038e+08\n",
            " 1.07132846e+08 7.04065800e+07 7.75726914e+07 9.36664814e+07]\n",
            "\n",
            "--- IPqwery - Patents Granted ---\n",
            "Unique count: 107\n",
            "Sample values:\n",
            "['0.0' '1.0' '0' '67' '2.0' '9.0' '22.0' '12.0' '4.0' '13.0' '7.0' '6'\n",
            " '8.0' '3' '19.0' '6.0' '2' '16.0' '3.0' '37.0']\n",
            "\n",
            "--- G2 Stack - Total Products Active ---\n",
            "Unique count: 83\n",
            "Sample values:\n",
            "[37.  8. 18.  6. 16. 13. 21.  7. 17. 31. 29.  5.  4. 10. 70. 24.  9. 14.\n",
            " 22. 30.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df_cleaned = pd.read_csv('/content/Canada_cleaned.csv')\n",
        "\n",
        "# List of numeric target columns to impute\n",
        "target_cols = [\n",
        "    \"Price\",\n",
        "    \"Valuation at IPO\",\n",
        "    \"IPqwery - Patents Granted\",\n",
        "    \"G2 Stack - Total Products Active\"\n",
        "]\n",
        "\n",
        "# Features we can use for prediction (choose generic ones available for all rows)\n",
        "base_features = [\n",
        "    \"Total Funding Amount\",\n",
        "    \"Last Funding Amount\",\n",
        "    \"Estimated Revenue Range\",\n",
        "    \"CB Rank (Company)\",\n",
        "    \"Number of Employees\",\n",
        "    \"Industries\"\n",
        "]\n",
        "\n",
        "cat_features = [\"Estimated Revenue Range\", \"Industries\"]\n",
        "\n",
        "for target_col in target_cols:\n",
        "    print(f\"\\nüöÄ Processing {target_col} ...\")\n",
        "\n",
        "    # Step 1: Force numeric\n",
        "    df_cleaned[target_col] = pd.to_numeric(\n",
        "        df_cleaned[target_col].astype(str).str.replace(\",\", \"\").str.strip(),\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "    # Step 2: Train/predict split\n",
        "    train_df = df_cleaned.dropna(subset=base_features + [target_col]).copy()\n",
        "    test_df = df_cleaned[df_cleaned[target_col].isnull()].copy()\n",
        "\n",
        "    if len(train_df) < 5:\n",
        "        print(f\"‚ö†Ô∏è Skipping {target_col} ‚Äî not enough rows to train ({len(train_df)} available).\")\n",
        "        continue\n",
        "\n",
        "    # Step 3: Handle categoricals\n",
        "    for col in cat_features:\n",
        "        train_df.loc[:, col] = train_df[col].astype(str).fillna(\"Unknown\")\n",
        "        test_df.loc[:, col] = test_df[col].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "    train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "    test_df.loc[:, cat_features] = encoder.transform(test_df[cat_features])\n",
        "\n",
        "    # Step 4: Train model\n",
        "    X_train = train_df[base_features]\n",
        "    y_train = train_df[target_col]\n",
        "    X_test = test_df[base_features]\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 5: Predict and insert\n",
        "    preds = model.predict(X_test)\n",
        "    df_cleaned.loc[test_df.index, target_col] = preds\n",
        "\n",
        "    print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")\n",
        "\n",
        "# Save results\n",
        "df_cleaned.to_csv(\"/content/Canada_cleaned.csv\", index=False)\n",
        "print(\"\\nüíæ All imputations complete and saved to /content/Canada_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "_E2aPIjhnrdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2b1f64-6e88-4761-df6d-a29105778f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Processing Price ...\n",
            "‚úÖ Missing after imputation for Price: 0\n",
            "\n",
            "üöÄ Processing Valuation at IPO ...\n",
            "‚úÖ Missing after imputation for Valuation at IPO: 0\n",
            "\n",
            "üöÄ Processing IPqwery - Patents Granted ...\n",
            "‚úÖ Missing after imputation for IPqwery - Patents Granted: 0\n",
            "\n",
            "üöÄ Processing G2 Stack - Total Products Active ...\n",
            "‚úÖ Missing after imputation for G2 Stack - Total Products Active: 0\n",
            "\n",
            "üíæ All imputations complete and saved to /content/Canada_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üßº Check missing values count and percentage\n",
        "missing_info = (\n",
        "    df_cleaned.isnull().sum()\n",
        "    .to_frame(\"Missing Count\")\n",
        "    .assign(\n",
        "        Missing_Percentage=lambda x: (x[\"Missing Count\"] / len(df_cleaned)) * 100\n",
        "    )\n",
        "    .sort_values(by=\"Missing Count\", ascending=False)\n",
        ")\n",
        "\n",
        "# üñ®Ô∏è Show only columns with missing values\n",
        "missing_info = missing_info[missing_info[\"Missing Count\"] > 0]\n",
        "print(missing_info)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "F5aXkdsibWoN",
        "outputId": "239c211a-c079-45e4-8fa0-952f44b94269"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_cleaned' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3392481241.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# üßº Check missing values count and percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m missing_info = (\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     .assign(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_cleaned' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7rN6GP-gpWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def process_country_data(country_name, file_path):\n",
        "    \"\"\"\n",
        "    Cleans and imputes missing values for a given country's dataset.\n",
        "\n",
        "    Args:\n",
        "        country_name (str): The name of the country.\n",
        "        file_path (str): The path to the country's CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"--- üåé Processing {country_name} data... ---\")\n",
        "\n",
        "    # Step 1: Load dataset and select core columns\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_path} not found. Skipping {country_name}.\")\n",
        "        return\n",
        "\n",
        "    important_columns = [\n",
        "        \"Founded Date\", \"Number of Founders\", \"Company Type\", \"Number of Employees\",\n",
        "        \"Industries\", \"Headquarters Location\", \"Headquarters Regions\", \"Number of Investors\",\n",
        "        \"Actively Hiring\", \"Number of Funding Rounds\", \"Last Funding Amount\",\n",
        "        \"Funding Status\", \"Last Funding Type\", \"Estimated Revenue Range\", \"IPqwery - Patents Granted\",\n",
        "        \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"SEMrush - Page Views / Visit\",\n",
        "        \"SEMrush - Bounce Rate\", \"Number of Events\", \"BuiltWith - Active Tech Count\",\n",
        "        \"G2 Stack - Total Products Active\", \"Number of Articles\", \"CB Rank (Company)\",\n",
        "        \"Total Funding Amount\", \"Valuation at IPO\", \"Price\", \"Number of Exits\",\n",
        "        \"Industry Groups\", \"Apptopia - Downloads Last 30 Days\", \"Apptopia - Number of Apps\"\n",
        "    ]\n",
        "\n",
        "    # Ensure all columns exist before trying to subset\n",
        "    available_cols = [col for col in important_columns if col in df.columns]\n",
        "    df_cleaned = df[available_cols].copy()\n",
        "\n",
        "    # Add back any missing columns with NaN values to maintain a consistent structure\n",
        "    for col in important_columns:\n",
        "        if col not in df_cleaned.columns:\n",
        "            df_cleaned[col] = np.nan\n",
        "\n",
        "    print(f\"‚úÖ Loaded data with shape: {df_cleaned.shape}\")\n",
        "\n",
        "    # Helper functions for imputation\n",
        "    def clean_numeric_col(series):\n",
        "        return pd.to_numeric(\n",
        "            series.astype(str).str.replace('[,%$]', '', regex=True).str.strip(),\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "    def fill_by_grouped_mode(df, target_col, group_col):\n",
        "        if target_col in df.columns and group_col in df.columns:\n",
        "            df[target_col] = df.groupby(group_col)[target_col].transform(\n",
        "                lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    def fill_by_grouped_median(df, target_col, group_col):\n",
        "        if target_col in df.columns and group_col in df.columns:\n",
        "            df[target_col] = df.groupby(group_col)[target_col].transform(\n",
        "                lambda x: x.fillna(x.median())\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    # Step 2: Impute columns with low missing values or simple logic\n",
        "\n",
        "    # CB Rank (Company)\n",
        "    if \"CB Rank (Company)\" in df_cleaned.columns:\n",
        "        df_cleaned[\"CB Rank (Company)\"] = clean_numeric_col(df_cleaned[\"CB Rank (Company)\"])\n",
        "        df_cleaned = fill_by_grouped_median(df_cleaned, \"CB Rank (Company)\", \"Industries\")\n",
        "        df_cleaned[\"CB Rank (Company)\"].fillna(df_cleaned[\"CB Rank (Company)\"].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed CB Rank (Company).\")\n",
        "\n",
        "    # Founded Date\n",
        "    if \"Founded Date\" in df_cleaned.columns:\n",
        "        df_cleaned['Founded Date'] = pd.to_datetime(df_cleaned['Founded Date'], errors='coerce')\n",
        "        df_cleaned = fill_by_grouped_median(df_cleaned, \"Founded Date\", \"Industries\")\n",
        "        df_cleaned['Founded Date'].fillna(df_cleaned['Founded Date'].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed Founded Date.\")\n",
        "\n",
        "    # Company Type\n",
        "    if \"Company Type\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Company Type\", \"Industries\")\n",
        "        df_cleaned[\"Company Type\"].fillna(df_cleaned[\"Company Type\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Company Type.\")\n",
        "\n",
        "    # Industries\n",
        "    if \"Industries\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Industries\", \"Company Type\")\n",
        "        df_cleaned[\"Industries\"].fillna(df_cleaned[\"Industries\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Industries.\")\n",
        "\n",
        "    # Number of Employees\n",
        "    if \"Number of Employees\" in df_cleaned.columns:\n",
        "        def parse_employees(val):\n",
        "            try:\n",
        "                val = str(val).replace(',', '')\n",
        "                if \"nan\" in val: return np.nan\n",
        "                if \"less than\" in val.lower(): return 1\n",
        "                if \"-\" in val:\n",
        "                    a, b = val.split(\"-\")\n",
        "                    return (int(a) + int(b)) // 2\n",
        "                return int(val)\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        df_cleaned[\"Number of Employees\"] = df_cleaned[\"Number of Employees\"].apply(parse_employees)\n",
        "        df_cleaned[\"Number of Employees\"].fillna(df_cleaned[\"Number of Employees\"].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed Number of Employees.\")\n",
        "\n",
        "    # Industry Groups\n",
        "    if \"Industry Groups\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Industry Groups\", \"Industries\")\n",
        "        df_cleaned[\"Industry Groups\"].fillna(df_cleaned[\"Industry Groups\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Industry Groups.\")\n",
        "\n",
        "    # Headquarters Regions\n",
        "    if \"Headquarters Regions\" in df_cleaned.columns:\n",
        "        def extract_region(location):\n",
        "            if pd.isnull(location): return np.nan\n",
        "            parts = location.split(\",\")\n",
        "            if len(parts) == 3:\n",
        "                return parts[1].strip()\n",
        "            return np.nan\n",
        "\n",
        "        df_cleaned['Extracted Region'] = df_cleaned['Headquarters Location'].apply(extract_region)\n",
        "        df_cleaned['Headquarters Regions'].fillna(df_cleaned['Extracted Region'], inplace=True)\n",
        "        df_cleaned['Headquarters Regions'].fillna('Unknown', inplace=True)\n",
        "        df_cleaned.drop(columns=['Extracted Region'], inplace=True, errors='ignore')\n",
        "        print(\"‚úÖ Imputed Headquarters Regions.\")\n",
        "\n",
        "    # Step 3: Impute using Machine Learning models\n",
        "\n",
        "    # Imputation function for regression\n",
        "    def ml_impute_regressor(df, target_col, features):\n",
        "        df_encoded = df.copy()\n",
        "        df_encoded[target_col] = clean_numeric_col(df_encoded[target_col])\n",
        "\n",
        "        cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "        train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "        predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "        if len(train_df) < 5 or predict_df.empty:\n",
        "            print(f\"‚ÑπÔ∏è Skipping ML imputation for {target_col} due to insufficient data.\")\n",
        "            if predict_df.empty:\n",
        "                print(\"‚úÖ All values are already present for this column.\")\n",
        "            return df\n",
        "\n",
        "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "        # Preprocess and encode\n",
        "        for col in cat_features:\n",
        "            train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "            predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "        predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model.fit(train_df[features], train_df[target_col])\n",
        "        y_pred = model.predict(predict_df[features])\n",
        "\n",
        "        df.loc[predict_df.index, target_col] = y_pred\n",
        "        return df\n",
        "\n",
        "    # Imputation function for classification\n",
        "    def ml_impute_classifier(df, target_col, features):\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "        cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "        train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "        predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "        if len(train_df) < 5 or predict_df.empty:\n",
        "            print(f\"‚ÑπÔ∏è Skipping ML imputation for {target_col} due to insufficient data.\")\n",
        "            if predict_df.empty:\n",
        "                print(\"‚úÖ All values are already present for this column.\")\n",
        "            return df\n",
        "\n",
        "        # Preprocess and encode\n",
        "        for col in cat_features:\n",
        "            train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "            predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "        train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "        predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(train_df[features], train_df[target_col])\n",
        "        y_pred = model.predict(predict_df[features])\n",
        "\n",
        "        df.loc[predict_df.index, target_col] = y_pred\n",
        "        return df\n",
        "\n",
        "    # List of columns to impute using ML\n",
        "    impute_list_reg = {\n",
        "        \"Estimated Revenue Range\": [\"Number of Employees\", \"CB Rank (Company)\", \"Funding Status\", \"Last Funding Amount\", \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"Industries\"],\n",
        "        \"Last Funding Amount\": [\"Estimated Revenue Range\", \"Number of Employees\", \"CB Rank (Company)\", \"Industries\", \"Funding Status\"],\n",
        "        \"Total Funding Amount\": [\"Last Funding Amount\", \"Estimated Revenue Range\", \"Number of Employees\", \"CB Rank (Company)\", \"Industries\", \"Funding Status\"],\n",
        "        \"Valuation at IPO\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Number of Articles\": [\"Number of Employees\", \"Total Funding Amount\", \"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Founders\"],\n",
        "        \"Number of Investors\": [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Total Funding Amount\", \"Number of Funding Rounds\", \"Last Funding Amount\"],\n",
        "        \"BuiltWith - Active Tech Count\": [\"Number of Employees\", \"Total Funding Amount\", \"Estimated Revenue Range\", \"Industry Groups\"],\n",
        "        \"G2 Stack - Total Products Active\": [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Employees\", \"Total Funding Amount\"],\n",
        "        \"IPqwery - Patents Granted\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Number of Events\": [\"Number of Employees\", \"Number of Articles\", \"Number of Funding Rounds\", \"Estimated Revenue Range\", \"Industries\"],\n",
        "        \"Number of Exits\": [\"Total Funding Amount\", \"Number of Employees\", \"Funding Status\", \"Industries\", \"Company Type\"],\n",
        "        \"Price\": [\"Total Funding Amount\", \"Valuation at IPO\", \"Number of Employees\", \"Funding Status\", \"Industries\"],\n",
        "        \"Number of Funding Rounds\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"Funding Status\", \"Industries\"],\n",
        "        \"Number of Founders\": [\"Company Type\", \"Industries\", \"Number of Employees\", \"Total Funding Amount\", \"Number of Articles\"]\n",
        "    }\n",
        "\n",
        "    impute_list_class = {\n",
        "        \"Funding Status\": [\"Estimated Revenue Range\", \"Last Funding Amount\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Last Funding Type\": [\"Funding Status\", \"Estimated Revenue Range\", \"Total Funding Amount\", \"Number of Funding Rounds\", \"Industries\", \"Last Funding Amount\"],\n",
        "        \"Actively Hiring\": [\"Number of Employees\", \"Estimated Revenue Range\", \"Funding Status\", \"Company Type\"]\n",
        "    }\n",
        "\n",
        "    # Special case for SEMrush and Apptopia columns\n",
        "    semrush_features = [\"Estimated Revenue Range\", \"Industries\", \"Number of Employees\", \"Last Funding Amount\"]\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Monthly Visits\", semrush_features)\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Page Views / Visit\", semrush_features + [\"SEMrush - Monthly Visits\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Visit Duration\", semrush_features + [\"SEMrush - Page Views / Visit\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Bounce Rate\", semrush_features + [\"SEMrush - Visit Duration\"])\n",
        "\n",
        "    apptopia_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Employees\", \"Apptopia - Number of Apps\"]\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Apptopia - Downloads Last 30 Days\", apptopia_features)\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Apptopia - Number of Apps\", apptopia_features + [\"Apptopia - Downloads Last 30 Days\"])\n",
        "\n",
        "    # Run ML imputation for all other columns\n",
        "    for col, features in impute_list_reg.items():\n",
        "        if col in df_cleaned.columns and all(f in df_cleaned.columns for f in features):\n",
        "            df_cleaned = ml_impute_regressor(df_cleaned, col, features)\n",
        "            print(f\"‚úÖ Imputed {col} using a regressor.\")\n",
        "\n",
        "    for col, features in impute_list_class.items():\n",
        "        if col in df_cleaned.columns and all(f in df_cleaned.columns for f in features):\n",
        "            df_cleaned = ml_impute_classifier(df_cleaned, col, features)\n",
        "            print(f\"‚úÖ Imputed {col} using a classifier.\")\n",
        "\n",
        "    # Final check and save\n",
        "    final_output_path = f\"/content/{country_name}_final_filled.csv\"\n",
        "    df_cleaned.to_csv(final_output_path, index=False)\n",
        "    print(f\"‚úÖ Final cleaned data for {country_name} saved to {final_output_path}\")\n",
        "    print(f\"--- üèÅ {country_name} processing complete. ---\\n\")\n",
        "\n",
        "# List of countries to process\n",
        "countries_to_process = [\n",
        "    ('Algeria', 'Algeria.csv'),\n",
        "    ('Angola', 'Angola.csv'),\n",
        "    ('Armenia', 'Armenia.csv'),\n",
        "    ('Australia', 'Australia.csv'),\n",
        "    ('Austria', 'Austria.csv'),\n",
        "    ('Belarus', 'Belarus.csv'),\n",
        "    ('Belize', 'Belize.csv'),\n",
        "    ('British_Indian_Ocean_Territory', 'British_Indian_Ocean_Territory.csv'),\n",
        "    ('Cameroon', 'Cameroon.csv'),\n",
        "    ('Canada', 'Canada.csv'),\n",
        "    ('Colombia', 'Colombia.csv'),\n",
        "    ('Congo', 'Congo.csv'),\n",
        "    ('Cyprus', 'Cyprus.csv'),\n",
        "    ('Ecuador', 'Ecuador.csv'),\n",
        "    ('Egypt', 'Egypt.csv'),\n",
        "    ('Equatorial_Guinea', 'Equatorial_Guinea.csv'),\n",
        "    ('Ethiopia', 'Ethiopia.csv'),\n",
        "    ('Faroe_Islands', 'Faroe_Islands.csv'),\n",
        "    ('Fiji', 'Fiji.csv'),\n",
        "    ('Finland', 'Finland.csv'),\n",
        "    ('Gabon', 'Gabon.csv'),\n",
        "    ('Ghana', 'Ghana.csv'),\n",
        "    ('Greece', 'Greece.csv'),\n",
        "    ('Guinea', 'Guinea.csv'),\n",
        "    ('Guyana', 'Guyana.csv'),\n",
        "    ('Honduras', 'Honduras.csv'),\n",
        "    ('Iceland', 'Iceland.csv'),\n",
        "    ('Indonesia', 'Indonesia.csv'),\n",
        "    ('Iraq', 'Iraq.csv'),\n",
        "    ('Isle_of_Man', 'Isle_of_Man.csv'),\n",
        "    ('Jersey', 'Jersey.csv'),\n",
        "    ('Malawi', 'Malawi.csv'),\n",
        "    ('Marshall_Islands', 'Marshall_Islands.csv'),\n",
        "    ('Mauritania', 'Mauritania.csv'),\n",
        "    ('Mauritius', 'Mauritius.csv')\n",
        "]\n",
        "\n",
        "# Run the process for each country\n",
        "# for country, file in countries_to_process:\n",
        "#     process_country_data(country, f\"/content/{file}\")\n",
        "process_country_data('Cyprus', \"Cyprus.csv\")"
      ],
      "metadata": {
        "id": "UAALGa83bXSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e0f40f-97ba-495d-d846-e7669e8d5be4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üåé Processing Cyprus data... ---\n",
            "‚úÖ Loaded data with shape: (1220, 31)\n",
            "‚úÖ Imputed CB Rank (Company).\n",
            "‚úÖ Imputed Founded Date.\n",
            "‚úÖ Imputed Company Type.\n",
            "‚úÖ Imputed Industries.\n",
            "‚úÖ Imputed Number of Employees.\n",
            "‚úÖ Imputed Industry Groups.\n",
            "‚úÖ Imputed Headquarters Regions.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Estimated Revenue Range due to insufficient data.\n",
            "‚úÖ Imputed Estimated Revenue Range using a regressor.\n",
            "‚úÖ Imputed Last Funding Amount using a regressor.\n",
            "‚úÖ Imputed Total Funding Amount using a regressor.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Valuation at IPO due to insufficient data.\n",
            "‚úÖ Imputed Valuation at IPO using a regressor.\n",
            "‚úÖ Imputed Number of Articles using a regressor.\n",
            "‚úÖ Imputed Number of Investors using a regressor.\n",
            "‚úÖ Imputed BuiltWith - Active Tech Count using a regressor.\n",
            "‚úÖ Imputed G2 Stack - Total Products Active using a regressor.\n",
            "‚úÖ Imputed IPqwery - Patents Granted using a regressor.\n",
            "‚úÖ Imputed Number of Events using a regressor.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Number of Exits due to insufficient data.\n",
            "‚úÖ Imputed Number of Exits using a regressor.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Price due to insufficient data.\n",
            "‚úÖ Imputed Price using a regressor.\n",
            "‚úÖ Imputed Number of Funding Rounds using a regressor.\n",
            "‚úÖ Imputed Number of Founders using a regressor.\n",
            "‚úÖ Imputed Funding Status using a classifier.\n",
            "‚úÖ Imputed Last Funding Type using a classifier.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Actively Hiring due to insufficient data.\n",
            "‚úÖ Imputed Actively Hiring using a classifier.\n",
            "‚úÖ Final cleaned data for Cyprus saved to /content/Cyprus_final_filled.csv\n",
            "--- üèÅ Cyprus processing complete. ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def process_country_data(country_name, file_path):\n",
        "    \"\"\"\n",
        "    Cleans and imputes missing values for a given country's dataset.\n",
        "\n",
        "    Args:\n",
        "        country_name (str): The name of the country.\n",
        "        file_path (str): The path to the country's CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"--- üåé Processing {country_name} data... ---\")\n",
        "\n",
        "    # Step 1: Load dataset and select core columns\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_path} not found. Skipping {country_name}.\")\n",
        "        return\n",
        "\n",
        "    important_columns = [\n",
        "        \"Founded Date\", \"Number of Founders\", \"Company Type\", \"Number of Employees\",\n",
        "        \"Industries\", \"Headquarters Location\", \"Headquarters Regions\", \"Number of Investors\",\n",
        "        \"Actively Hiring\", \"Number of Funding Rounds\", \"Last Funding Amount\",\n",
        "        \"Funding Status\", \"Last Funding Type\", \"Estimated Revenue Range\", \"IPqwery - Patents Granted\",\n",
        "        \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"SEMrush - Page Views / Visit\",\n",
        "        \"SEMrush - Bounce Rate\", \"Number of Events\", \"BuiltWith - Active Tech Count\",\n",
        "        \"G2 Stack - Total Products Active\", \"Number of Articles\", \"CB Rank (Company)\",\n",
        "        \"Total Funding Amount\", \"Valuation at IPO\", \"Price\", \"Number of Exits\",\n",
        "        \"Industry Groups\", \"Apptopia - Downloads Last 30 Days\", \"Apptopia - Number of Apps\"\n",
        "    ]\n",
        "\n",
        "    # Ensure all columns exist before trying to subset\n",
        "    available_cols = [col for col in important_columns if col in df.columns]\n",
        "    df_cleaned = df[available_cols].copy()\n",
        "\n",
        "    # Add back any missing columns with NaN values to maintain a consistent structure\n",
        "    for col in important_columns:\n",
        "        if col not in df_cleaned.columns:\n",
        "            df_cleaned[col] = np.nan\n",
        "\n",
        "    print(f\"‚úÖ Loaded data with shape: {df_cleaned.shape}\")\n",
        "\n",
        "    # Helper functions for imputation\n",
        "    def clean_numeric_col(series):\n",
        "        return pd.to_numeric(\n",
        "            series.astype(str).str.replace('[,%$]', '', regex=True).str.strip(),\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "    def fill_by_grouped_mode(df, target_col, group_col):\n",
        "        if target_col in df.columns and group_col in df.columns:\n",
        "            df[target_col] = df.groupby(group_col)[target_col].transform(\n",
        "                lambda x: x.fillna(x.mode().iloc[0]) if not x.mode().empty else x\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    def fill_by_grouped_median(df, target_col, group_col):\n",
        "        if target_col in df.columns and group_col in df.columns:\n",
        "            df[target_col] = df.groupby(group_col)[target_col].transform(\n",
        "                lambda x: x.fillna(x.median())\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    # Step 2: Impute columns with low missing values or simple logic\n",
        "\n",
        "    # CB Rank (Company)\n",
        "    if \"CB Rank (Company)\" in df_cleaned.columns:\n",
        "        df_cleaned[\"CB Rank (Company)\"] = clean_numeric_col(df_cleaned[\"CB Rank (Company)\"])\n",
        "        df_cleaned = fill_by_grouped_median(df_cleaned, \"CB Rank (Company)\", \"Industries\")\n",
        "        df_cleaned[\"CB Rank (Company)\"].fillna(df_cleaned[\"CB Rank (Company)\"].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed CB Rank (Company).\")\n",
        "\n",
        "    # Founded Date\n",
        "    if \"Founded Date\" in df_cleaned.columns:\n",
        "        df_cleaned['Founded Date'] = pd.to_datetime(df_cleaned['Founded Date'], errors='coerce')\n",
        "        df_cleaned = fill_by_grouped_median(df_cleaned, \"Founded Date\", \"Industries\")\n",
        "        df_cleaned['Founded Date'].fillna(df_cleaned['Founded Date'].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed Founded Date.\")\n",
        "\n",
        "    # Company Type\n",
        "    if \"Company Type\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Company Type\", \"Industries\")\n",
        "        df_cleaned[\"Company Type\"].fillna(df_cleaned[\"Company Type\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Company Type.\")\n",
        "\n",
        "    # Industries\n",
        "    if \"Industries\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Industries\", \"Company Type\")\n",
        "        df_cleaned[\"Industries\"].fillna(df_cleaned[\"Industries\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Industries.\")\n",
        "\n",
        "    # Number of Employees\n",
        "    if \"Number of Employees\" in df_cleaned.columns:\n",
        "        def parse_employees(val):\n",
        "            try:\n",
        "                val = str(val).replace(',', '')\n",
        "                if \"nan\" in val: return np.nan\n",
        "                if \"less than\" in val.lower(): return 1\n",
        "                if \"-\" in val:\n",
        "                    a, b = val.split(\"-\")\n",
        "                    return (int(a) + int(b)) // 2\n",
        "                return int(val)\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        df_cleaned[\"Number of Employees\"] = df_cleaned[\"Number of Employees\"].apply(parse_employees)\n",
        "        df_cleaned[\"Number of Employees\"].fillna(df_cleaned[\"Number of Employees\"].median(), inplace=True)\n",
        "        print(\"‚úÖ Imputed Number of Employees.\")\n",
        "\n",
        "    # Industry Groups\n",
        "    if \"Industry Groups\" in df_cleaned.columns:\n",
        "        df_cleaned = fill_by_grouped_mode(df_cleaned, \"Industry Groups\", \"Industries\")\n",
        "        df_cleaned[\"Industry Groups\"].fillna(df_cleaned[\"Industry Groups\"].mode().iloc[0], inplace=True)\n",
        "        print(\"‚úÖ Imputed Industry Groups.\")\n",
        "\n",
        "    # Headquarters Regions\n",
        "    if \"Headquarters Regions\" in df_cleaned.columns:\n",
        "        def extract_region(location):\n",
        "            if pd.isnull(location): return np.nan\n",
        "            parts = location.split(\",\")\n",
        "            if len(parts) == 3:\n",
        "                return parts[1].strip()\n",
        "            return np.nan\n",
        "\n",
        "        df_cleaned['Extracted Region'] = df_cleaned['Headquarters Location'].apply(extract_region)\n",
        "        df_cleaned['Headquarters Regions'].fillna(df_cleaned['Extracted Region'], inplace=True)\n",
        "        df_cleaned['Headquarters Regions'].fillna('Unknown', inplace=True)\n",
        "        df_cleaned.drop(columns=['Extracted Region'], inplace=True, errors='ignore')\n",
        "        print(\"‚úÖ Imputed Headquarters Regions.\")\n",
        "\n",
        "    # Step 3: Impute using Machine Learning models\n",
        "\n",
        "    # Imputation function for regression\n",
        "    def ml_impute_regressor(df, target_col, features, special_case=False):\n",
        "        df_encoded = df.copy()\n",
        "        df_encoded[target_col] = clean_numeric_col(df_encoded[target_col])\n",
        "\n",
        "        # Clean features\n",
        "        for col in features:\n",
        "            if df_encoded[col].dtype == 'object':\n",
        "                 df_encoded[col] = df_encoded[col].astype(str)\n",
        "            else:\n",
        "                df_encoded[col] = clean_numeric_col(df_encoded[col])\n",
        "\n",
        "        cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "        train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "        predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "        if len(train_df) < 5 or predict_df.empty:\n",
        "            print(f\"‚ÑπÔ∏è Skipping ML imputation for {target_col} due to insufficient data or no missing values.\")\n",
        "            return df\n",
        "\n",
        "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "        # Preprocess and encode\n",
        "        for col in cat_features:\n",
        "            train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "            predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "        predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model.fit(train_df[features], train_df[target_col])\n",
        "        y_pred = model.predict(predict_df[features])\n",
        "\n",
        "        # If the target column should be an integer, round the predictions\n",
        "        if 'Number' in target_col or target_col in ['IPqwery - Patents Granted', 'G2 Stack - Total Products Active']:\n",
        "            y_pred = np.round(y_pred)\n",
        "\n",
        "        df.loc[predict_df.index, target_col] = y_pred\n",
        "        return df\n",
        "\n",
        "    # Imputation function for classification\n",
        "    def ml_impute_classifier(df, target_col, features):\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "        cat_features = [col for col in features if df_encoded[col].dtype == 'object']\n",
        "\n",
        "        train_df = df_encoded.dropna(subset=features + [target_col])\n",
        "        predict_df = df_encoded[df_encoded[target_col].isnull()]\n",
        "\n",
        "        if len(train_df) < 5 or predict_df.empty:\n",
        "            print(f\"‚ÑπÔ∏è Skipping ML imputation for {target_col} due to insufficient data or no missing values.\")\n",
        "            return df\n",
        "\n",
        "        # Preprocess and encode\n",
        "        for col in cat_features:\n",
        "            train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "            predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "        train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "        predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(train_df[features], train_df[target_col])\n",
        "        y_pred = model.predict(predict_df[features])\n",
        "\n",
        "        df.loc[predict_df.index, target_col] = y_pred\n",
        "        return df\n",
        "\n",
        "    # List of columns to impute using ML\n",
        "    impute_list_reg = {\n",
        "        \"Last Funding Amount\": [\"Estimated Revenue Range\", \"Number of Employees\", \"CB Rank (Company)\", \"Industries\", \"Funding Status\"],\n",
        "        \"Total Funding Amount\": [\"Last Funding Amount\", \"Estimated Revenue Range\", \"Number of Employees\", \"CB Rank (Company)\", \"Industries\", \"Funding Status\"],\n",
        "        \"Valuation at IPO\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Number of Articles\": [\"Number of Employees\", \"Total Funding Amount\", \"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Founders\"],\n",
        "        \"Number of Investors\": [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Total Funding Amount\", \"Number of Funding Rounds\", \"Last Funding Amount\"],\n",
        "        \"BuiltWith - Active Tech Count\": [\"Number of Employees\", \"Total Funding Amount\", \"Estimated Revenue Range\", \"Industry Groups\"],\n",
        "        \"G2 Stack - Total Products Active\": [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Employees\", \"Total Funding Amount\"],\n",
        "        \"IPqwery - Patents Granted\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Number of Events\": [\"Number of Employees\", \"Number of Articles\", \"Number of Funding Rounds\", \"Estimated Revenue Range\", \"Industries\"],\n",
        "        \"Number of Exits\": [\"Total Funding Amount\", \"Number of Employees\", \"Funding Status\", \"Industries\", \"Company Type\"],\n",
        "        \"Price\": [\"Total Funding Amount\", \"Valuation at IPO\", \"Number of Employees\", \"Funding Status\", \"Industries\"],\n",
        "        \"Number of Funding Rounds\": [\"Total Funding Amount\", \"Last Funding Amount\", \"Estimated Revenue Range\", \"Funding Status\", \"Industries\"],\n",
        "        \"Number of Founders\": [\"Company Type\", \"Industries\", \"Number of Employees\", \"Total Funding Amount\", \"Number of Articles\"]\n",
        "    }\n",
        "\n",
        "    impute_list_class = {\n",
        "        \"Estimated Revenue Range\": [\"Number of Employees\", \"CB Rank (Company)\", \"Funding Status\", \"Last Funding Amount\", \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"Industries\"],\n",
        "        \"Funding Status\": [\"Estimated Revenue Range\", \"Last Funding Amount\", \"CB Rank (Company)\", \"Number of Employees\", \"Industries\"],\n",
        "        \"Last Funding Type\": [\"Funding Status\", \"Estimated Revenue Range\", \"Total Funding Amount\", \"Number of Funding Rounds\", \"Industries\", \"Last Funding Amount\"],\n",
        "        \"Actively Hiring\": [\"Number of Employees\", \"Estimated Revenue Range\", \"Funding Status\", \"Company Type\"]\n",
        "    }\n",
        "\n",
        "    # Order of Imputation:\n",
        "    # 1. First, impute all columns that are used as features for others.\n",
        "    # 2. Then, impute other columns.\n",
        "\n",
        "    # Impute `Last Funding Amount` and `Total Funding Amount` first, as they are crucial for `Estimated Revenue Range`\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Last Funding Amount\", impute_list_reg[\"Last Funding Amount\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Total Funding Amount\", impute_list_reg[\"Total Funding Amount\"])\n",
        "\n",
        "    # New Logic: Overwrite Estimated Revenue Range based on Total Funding Amount where it was missing\n",
        "    if \"Estimated Revenue Range\" in df_cleaned.columns and \"Total Funding Amount\" in df_cleaned.columns:\n",
        "        null_revenue_mask = df_cleaned[\"Estimated Revenue Range\"].isnull()\n",
        "\n",
        "        # Apply new logic only to the previously missing rows\n",
        "        df_cleaned.loc[null_revenue_mask & (df_cleaned[\"Total Funding Amount\"] < 1000000), \"Estimated Revenue Range\"] = 'Less than $1M'\n",
        "        df_cleaned.loc[null_revenue_mask & (df_cleaned[\"Total Funding Amount\"].between(1000000, 10000000)), \"Estimated Revenue Range\"] = '$1M to $10M'\n",
        "        df_cleaned.loc[null_revenue_mask & (df_cleaned[\"Total Funding Amount\"] > 10000000), \"Estimated Revenue Range\"] = '$10M+'\n",
        "\n",
        "        # Handle any remaining missing values with the old classifier model\n",
        "        df_cleaned = ml_impute_classifier(df_cleaned, \"Estimated Revenue Range\", impute_list_class[\"Estimated Revenue Range\"])\n",
        "        print(\"‚úÖ Imputed Estimated Revenue Range with new funding-based logic.\")\n",
        "\n",
        "    # Impute other core columns\n",
        "    df_cleaned = ml_impute_classifier(df_cleaned, \"Funding Status\", impute_list_class[\"Funding Status\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Number of Funding Rounds\", impute_list_reg[\"Number of Funding Rounds\"])\n",
        "    df_cleaned = ml_impute_classifier(df_cleaned, \"Last Funding Type\", impute_list_class[\"Last Funding Type\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Number of Founders\", impute_list_reg[\"Number of Founders\"])\n",
        "\n",
        "    # Special case for SEMrush and Apptopia columns\n",
        "    semrush_features = [\"Estimated Revenue Range\", \"Industries\", \"Number of Employees\", \"Last Funding Amount\"]\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Monthly Visits\", semrush_features)\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Page Views / Visit\", semrush_features + [\"SEMrush - Monthly Visits\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Visit Duration\", semrush_features + [\"SEMrush - Page Views / Visit\"])\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"SEMrush - Bounce Rate\", semrush_features + [\"SEMrush - Visit Duration\"])\n",
        "    print(\"‚úÖ Imputed SEMrush columns.\")\n",
        "\n",
        "    apptopia_features = [\"Estimated Revenue Range\", \"Industries\", \"Funding Status\", \"Number of Employees\", \"Apptopia - Number of Apps\"]\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Apptopia - Downloads Last 30 Days\", apptopia_features)\n",
        "    df_cleaned = ml_impute_regressor(df_cleaned, \"Apptopia - Number of Apps\", apptopia_features + [\"Apptopia - Downloads Last 30 Days\"])\n",
        "    print(\"‚úÖ Imputed Apptopia columns.\")\n",
        "\n",
        "    # Impute remaining columns\n",
        "    remaining_cols = [\n",
        "        \"Valuation at IPO\", \"Number of Articles\", \"Number of Investors\",\n",
        "        \"BuiltWith - Active Tech Count\", \"G2 Stack - Total Products Active\",\n",
        "        \"IPqwery - Patents Granted\", \"Number of Events\", \"Number of Exits\",\n",
        "        \"Price\", \"Actively Hiring\"\n",
        "    ]\n",
        "    for col in remaining_cols:\n",
        "        if col in impute_list_reg:\n",
        "            df_cleaned = ml_impute_regressor(df_cleaned, col, impute_list_reg[col])\n",
        "            print(f\"‚úÖ Imputed {col}.\")\n",
        "        elif col in impute_list_class:\n",
        "            df_cleaned = ml_impute_classifier(df_cleaned, col, impute_list_class[col])\n",
        "            print(f\"‚úÖ Imputed {col}.\")\n",
        "\n",
        "    # New Logic: Round numeric columns to nearest whole number\n",
        "    numeric_cols_to_round = [\n",
        "        \"Number of Founders\", \"Number of Employees\", \"Number of Investors\",\n",
        "        \"Number of Funding Rounds\", \"IPqwery - Patents Granted\",\n",
        "        \"SEMrush - Monthly Visits\", \"SEMrush - Visit Duration\", \"SEMrush - Page Views / Visit\",\n",
        "        \"SEMrush - Bounce Rate\", \"Number of Events\", \"BuiltWith - Active Tech Count\",\n",
        "        \"G2 Stack - Total Products Active\", \"Number of Articles\",\n",
        "        \"Number of Exits\", \"Apptopia - Downloads Last 30 Days\", \"Apptopia - Number of Apps\"\n",
        "    ]\n",
        "\n",
        "    for col in numeric_cols_to_round:\n",
        "        if col in df_cleaned.columns:\n",
        "            # Check if the column is of a float type before rounding\n",
        "            if pd.api.types.is_numeric_dtype(df_cleaned[col]):\n",
        "                df_cleaned[col] = df_cleaned[col].round(0).astype('Int64')\n",
        "    print(\"‚úÖ Rounded specified numeric columns to the nearest whole number.\")\n",
        "\n",
        "    # Final check and save\n",
        "    final_output_path = f\"/content/{country_name}_final_filled.csv\"\n",
        "    df_cleaned.to_csv(final_output_path, index=False)\n",
        "    print(f\"‚úÖ Final cleaned data for {country_name} saved to {final_output_path}\")\n",
        "    print(f\"--- üèÅ {country_name} processing complete. ---\\n\")\n",
        "\n",
        "# List of countries to process\n",
        "countries_to_process = [\n",
        "    ('Algeria', 'Algeria.csv'),\n",
        "    ('Angola', 'Angola.csv'),\n",
        "    ('Armenia', 'Armenia.csv'),\n",
        "    ('Australia', 'Australia.csv'),\n",
        "    ('Austria', 'Austria.csv'),\n",
        "    ('Belarus', 'Belarus.csv'),\n",
        "    ('Belize', 'Belize.csv'),\n",
        "    ('British_Indian_Ocean_Territory', 'British_Indian_Ocean_Territory.csv'),\n",
        "    ('Cameroon', 'Cameroon.csv'),\n",
        "    ('Canada', 'Canada.csv'),\n",
        "    ('Colombia', 'Colombia.csv'),\n",
        "    ('Congo', 'Congo.csv'),\n",
        "    ('Ecuador', 'Ecuador.csv'),\n",
        "    ('Egypt', 'Egypt.csv'),\n",
        "    ('Equatorial_Guinea', 'Equatorial_Guinea.csv'),\n",
        "    ('Ethiopia', 'Ethiopia.csv'),\n",
        "    ('Faroe_Islands', 'Faroe_Islands.csv'),\n",
        "    ('Fiji', 'Fiji.csv'),\n",
        "    ('Finland', 'Finland.csv'),\n",
        "    ('Gabon', 'Gabon.csv'),\n",
        "    ('Ghana', 'Ghana.csv'),\n",
        "    ('Greece', 'Greece.csv'),\n",
        "    ('Guinea', 'Guinea.csv'),\n",
        "    ('Guyana', 'Guyana.csv'),\n",
        "    ('Honduras', 'Honduras.csv'),\n",
        "    ('Iceland', 'Iceland.csv'),\n",
        "    ('Indonesia', 'Indonesia.csv'),\n",
        "    ('Iraq', 'Iraq.csv'),\n",
        "    ('Isle_of_Man', 'Isle_of_Man.csv'),\n",
        "    ('Jersey', 'Jersey.csv'),\n",
        "    ('Malawi', 'Malawi.csv'),\n",
        "    ('Marshall_Islands', 'Marshall_Islands.csv'),\n",
        "    ('Mauritania', 'Mauritania.csv'),\n",
        "    ('Mauritius', 'Mauritius.csv')\n",
        "]\n",
        "\n",
        "# Run the process for each country\n",
        "for country, file in countries_to_process:\n",
        "    process_country_data(country, f\"/content/{file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45bgxgj1gqy_",
        "outputId": "1204907b-9057-4659-9d01-0bca2d842761"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üåé Processing Italy data... ---\n",
            "‚úÖ Loaded data with shape: (8269, 31)\n",
            "‚úÖ Imputed CB Rank (Company).\n",
            "‚úÖ Imputed Founded Date.\n",
            "‚úÖ Imputed Company Type.\n",
            "‚úÖ Imputed Industries.\n",
            "‚úÖ Imputed Number of Employees.\n",
            "‚úÖ Imputed Industry Groups.\n",
            "‚úÖ Imputed Headquarters Regions.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Estimated Revenue Range due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Estimated Revenue Range with new funding-based logic.\n",
            "‚úÖ Imputed SEMrush columns.\n",
            "‚úÖ Imputed Apptopia columns.\n",
            "‚úÖ Imputed Valuation at IPO.\n",
            "‚úÖ Imputed Number of Articles.\n",
            "‚úÖ Imputed Number of Investors.\n",
            "‚úÖ Imputed BuiltWith - Active Tech Count.\n",
            "‚úÖ Imputed G2 Stack - Total Products Active.\n",
            "‚úÖ Imputed IPqwery - Patents Granted.\n",
            "‚úÖ Imputed Number of Events.\n",
            "‚úÖ Imputed Number of Exits.\n",
            "‚úÖ Imputed Price.\n",
            "‚úÖ Imputed Actively Hiring.\n",
            "‚úÖ Rounded specified numeric columns to the nearest whole number.\n",
            "‚úÖ Final cleaned data for Italy saved to /content/Italy_final_filled.csv\n",
            "--- üèÅ Italy processing complete. ---\n",
            "\n",
            "--- üåé Processing C√¥te_dIvoire data... ---\n",
            "‚úÖ Loaded data with shape: (131, 31)\n",
            "‚úÖ Imputed CB Rank (Company).\n",
            "‚úÖ Imputed Founded Date.\n",
            "‚úÖ Imputed Company Type.\n",
            "‚úÖ Imputed Industries.\n",
            "‚úÖ Imputed Number of Employees.\n",
            "‚úÖ Imputed Industry Groups.\n",
            "‚úÖ Imputed Headquarters Regions.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Estimated Revenue Range due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Estimated Revenue Range with new funding-based logic.\n",
            "‚úÖ Imputed SEMrush columns.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Apptopia - Downloads Last 30 Days due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Apptopia columns.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Valuation at IPO due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Valuation at IPO.\n",
            "‚úÖ Imputed Number of Articles.\n",
            "‚úÖ Imputed Number of Investors.\n",
            "‚úÖ Imputed BuiltWith - Active Tech Count.\n",
            "‚úÖ Imputed G2 Stack - Total Products Active.\n",
            "‚ÑπÔ∏è Skipping ML imputation for IPqwery - Patents Granted due to insufficient data or no missing values.\n",
            "‚úÖ Imputed IPqwery - Patents Granted.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Number of Events due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Number of Events.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Number of Exits due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Number of Exits.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Price due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Price.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Actively Hiring due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Actively Hiring.\n",
            "‚úÖ Rounded specified numeric columns to the nearest whole number.\n",
            "‚úÖ Final cleaned data for C√¥te_dIvoire saved to /content/C√¥te_dIvoire_final_filled.csv\n",
            "--- üèÅ C√¥te_dIvoire processing complete. ---\n",
            "\n",
            "--- üåé Processing Germany data... ---\n",
            "‚úÖ Loaded data with shape: (15364, 31)\n",
            "‚úÖ Imputed CB Rank (Company).\n",
            "‚úÖ Imputed Founded Date.\n",
            "‚úÖ Imputed Company Type.\n",
            "‚úÖ Imputed Industries.\n",
            "‚úÖ Imputed Number of Employees.\n",
            "‚úÖ Imputed Industry Groups.\n",
            "‚úÖ Imputed Headquarters Regions.\n",
            "‚ÑπÔ∏è Skipping ML imputation for Estimated Revenue Range due to insufficient data or no missing values.\n",
            "‚úÖ Imputed Estimated Revenue Range with new funding-based logic.\n",
            "‚úÖ Imputed SEMrush columns.\n",
            "‚úÖ Imputed Apptopia columns.\n",
            "‚úÖ Imputed Valuation at IPO.\n",
            "‚úÖ Imputed Number of Articles.\n",
            "‚úÖ Imputed Number of Investors.\n",
            "‚úÖ Imputed BuiltWith - Active Tech Count.\n",
            "‚úÖ Imputed G2 Stack - Total Products Active.\n",
            "‚úÖ Imputed IPqwery - Patents Granted.\n",
            "‚úÖ Imputed Number of Events.\n",
            "‚úÖ Imputed Number of Exits.\n",
            "‚úÖ Imputed Price.\n",
            "‚úÖ Imputed Actively Hiring.\n",
            "‚úÖ Rounded specified numeric columns to the nearest whole number.\n",
            "‚úÖ Final cleaned data for Germany saved to /content/Germany_final_filled.csv\n",
            "--- üèÅ Germany processing complete. ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def impute_actively_hiring(file_path):\n",
        "    \"\"\"\n",
        "    Imputes the 'Actively Hiring' column using RandomForestClassifier\n",
        "    and saves the updated data back to the same file.\n",
        "    Includes a fallback logic for when there is insufficient training data.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file to process.\n",
        "    \"\"\"\n",
        "    print(f\"--- Processing {os.path.basename(file_path)} ---\")\n",
        "    try:\n",
        "        df_cleaned = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_path} not found. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # --- Setup for Imputation ---\n",
        "    target_col = \"Actively Hiring\"\n",
        "    features = [\n",
        "        \"Number of Employees\",\n",
        "        \"Estimated Revenue Range\",\n",
        "        \"Funding Status\",\n",
        "        \"Company Type\"\n",
        "    ]\n",
        "\n",
        "    # Check if necessary columns exist\n",
        "    if not all(col in df_cleaned.columns for col in features + [target_col]):\n",
        "        print(f\"‚ö†Ô∏è Required columns are missing in {os.path.basename(file_path)}. Skipping imputation.\")\n",
        "        return\n",
        "\n",
        "    # Create a copy for encoding\n",
        "    df_encoded = df_cleaned.copy()\n",
        "\n",
        "    # --- Preprocessing and Encoding ---\n",
        "    # Identify categorical features\n",
        "    cat_features = [col for col in features if col in df_encoded.columns and df_encoded[col].dtype == 'object']\n",
        "\n",
        "    # Prepare training data: remove rows where target is missing\n",
        "    train_df = df_encoded.dropna(subset=features + [target_col]).copy()\n",
        "    predict_df = df_encoded[df_encoded[target_col].isnull()].copy()\n",
        "\n",
        "    if predict_df.empty:\n",
        "        print(\"‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\")\n",
        "        return\n",
        "\n",
        "    if train_df.empty:\n",
        "        print(f\"‚ö†Ô∏è No sufficient data to train a model for '{target_col}'. Applying fallback logic.\")\n",
        "        # --- NEW FALLBACK LOGIC ---\n",
        "        # Ensure Total Funding Amount is numeric for the fallback logic\n",
        "        if \"Total Funding Amount\" in predict_df.columns:\n",
        "            predict_df.loc[:, \"Total Funding Amount\"] = pd.to_numeric(\n",
        "                predict_df[\"Total Funding Amount\"].astype(str).str.replace('[,%$]', '', regex=True).str.strip(),\n",
        "                errors='coerce'\n",
        "            )\n",
        "            # Apply the funding-based rule\n",
        "            predict_df.loc[predict_df[\"Total Funding Amount\"] < 10000, target_col] = 'No'\n",
        "            predict_df.loc[predict_df[\"Total Funding Amount\"] >= 10000, target_col] = 'Yes'\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è 'Total Funding Amount' is missing. Falling back to default 'No'.\")\n",
        "            predict_df.loc[:, target_col] = 'No'\n",
        "\n",
        "        # Fill the original DataFrame with the predicted values and save\n",
        "        df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = predict_df[target_col]\n",
        "        df_cleaned.to_csv(file_path, index=False)\n",
        "        print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")\n",
        "        print(f\"üíæ Updated data saved back to {os.path.basename(file_path)}\")\n",
        "        print(\"-\" * 30)\n",
        "        return\n",
        "\n",
        "    # Handle 'nan' values in categorical features for encoding\n",
        "    for col in cat_features:\n",
        "        train_df.loc[:, col] = train_df[col].astype(str).fillna('Unknown')\n",
        "        predict_df.loc[:, col] = predict_df[col].astype(str).fillna('Unknown')\n",
        "\n",
        "    # Use OrdinalEncoder to transform categorical features\n",
        "    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    train_df.loc[:, cat_features] = encoder.fit_transform(train_df[cat_features])\n",
        "    predict_df.loc[:, cat_features] = encoder.transform(predict_df[cat_features])\n",
        "\n",
        "    # --- Train and Predict ---\n",
        "    X_train = train_df[features]\n",
        "    y_train = train_df[target_col]\n",
        "    X_pred = predict_df[features]\n",
        "\n",
        "    # Train a RandomForestClassifier model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the missing values\n",
        "    y_pred = model.predict(X_pred)\n",
        "\n",
        "    # --- Impute and Save ---\n",
        "    # Fill missing values and save to the same file, overwriting the original\n",
        "    df_cleaned.loc[df_cleaned[target_col].isnull(), target_col] = y_pred\n",
        "    df_cleaned.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Missing after imputation for {target_col}: {df_cleaned[target_col].isnull().sum()}\")\n",
        "    print(f\"üíæ Updated data saved back to {os.path.basename(file_path)}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# List of countries to process, assuming cleaned files exist\n",
        "countries_to_process = [\n",
        "    'Algeria', 'Angola', 'Armenia', 'Australia', 'Austria', 'Belarus', 'Belize',\n",
        "    'British_Indian_Ocean_Territory', 'Cameroon', 'Canada', 'Colombia', 'Congo',\n",
        "    'Cayman_Islands', 'Cyprus', 'C√¥te_dIvoire', 'Germany', 'Italy', 'Ecuador',\n",
        "    'Egypt', 'Equatorial_Guinea', 'Ethiopia', 'Faroe_Islands', 'Fiji', 'Finland',\n",
        "    'Gabon', 'Ghana', 'Greece', 'Guinea', 'Guyana', 'Honduras', 'Iceland',\n",
        "    'Indonesia', 'Iraq', 'Isle_of_Man', 'Jersey', 'Malawi', 'Marshall_Islands',\n",
        "    'Mauritania', 'Mauritius'\n",
        "]\n",
        "\n",
        "# Run the process for each country\n",
        "for country in countries_to_process:\n",
        "    impute_actively_hiring(f'/content/{country}_final_filled.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OofKhEJgtkfh",
        "outputId": "d92c865c-f85b-4048-ceca-2970fdbc25f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing Algeria_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Algeria_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Angola_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 4\n",
            "üíæ Updated data saved back to Angola_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Armenia_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Armenia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Australia_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Austria_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Belarus_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Belize_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Belize_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing British_Indian_Ocean_Territory_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to British_Indian_Ocean_Territory_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cameroon_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Cameroon_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Canada_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Colombia_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Congo_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 9\n",
            "üíæ Updated data saved back to Congo_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cayman_Islands_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Cayman_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cyprus_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing C√¥te_dIvoire_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to C√¥te_dIvoire_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Germany_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Italy_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Ecuador_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Ecuador_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Egypt_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Equatorial_Guinea_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 4\n",
            "üíæ Updated data saved back to Equatorial_Guinea_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Ethiopia_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Ethiopia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Faroe_Islands_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 1\n",
            "üíæ Updated data saved back to Faroe_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Fiji_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 8\n",
            "üíæ Updated data saved back to Fiji_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Finland_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Gabon_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 10\n",
            "üíæ Updated data saved back to Gabon_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Ghana_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Greece_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Guinea_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 14\n",
            "üíæ Updated data saved back to Guinea_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Guyana_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 17\n",
            "üíæ Updated data saved back to Guyana_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Honduras_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Honduras_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Iceland_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Iceland_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Indonesia_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Iraq_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Iraq_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Isle_of_Man_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Jersey_final_filled.csv ---\n",
            "‚úÖ No missing values for 'Actively Hiring'. Skipping imputation.\n",
            "--- Processing Malawi_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 41\n",
            "üíæ Updated data saved back to Malawi_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Marshall_Islands_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Marshall_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Mauritania_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 3\n",
            "üíæ Updated data saved back to Mauritania_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Mauritius_final_filled.csv ---\n",
            "‚ö†Ô∏è No sufficient data to train a model for 'Actively Hiring'. Applying fallback logic.\n",
            "‚úÖ Missing after imputation for Actively Hiring: 0\n",
            "üíæ Updated data saved back to Mauritius_final_filled.csv\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def drop_specified_columns(file_path):\n",
        "    \"\"\"\n",
        "    Drops a predefined list of columns from a CSV file and saves the updated\n",
        "    data back to the same file, overwriting the original.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file to process.\n",
        "    \"\"\"\n",
        "    print(f\"--- Processing {os.path.basename(file_path)} ---\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: {file_path} not found. Skipping.\")\n",
        "        return\n",
        "\n",
        "    columns_to_drop = [\n",
        "        \"Company Type\",\n",
        "        \"IPqwery - Patents Granted\",\n",
        "        \"SEMrush - Monthly Visits\",\n",
        "        \"SEMrush - Visit Duration\",\n",
        "        \"SEMrush - Page Views / Visit\",\n",
        "        \"SEMrush - Bounce Rate\",\n",
        "        \"Number of Articles\",\n",
        "        \"CB Rank (Company)\",\n",
        "        \"Valuation at IPO\",\n",
        "        \"Price\",  # This column may not exist from the last script\n",
        "        \"Number of Exits\",\n",
        "        \"Diversity Spotlight (US Headquarters Only)\",\n",
        "        \"Industry Groups\",\n",
        "        \"Apptopia - Downloads Last 30 Days\",\n",
        "        \"Apptopia - Number of Apps\",\n",
        "        \"SEMrush - Global Traffic Rank\", # This column may not exist from the last script\n",
        "        \"Investor Type\", # This column may not exist from the last script\n",
        "        \"Investment Stage\", # This column may not exist from the last script\n",
        "        \"Number of Portfolio Organizations\", # This column may not exist from the last script\n",
        "        \"IPqwery - Trademarks Registered\", # This column may not exist from the last script\n",
        "        \"IPqwery - Most Popular Patent Class\", # This column may not exist from the last script\n",
        "        \"IPqwery - Most Popular Trademark Class\", # This column may not exist from the last script\n",
        "        \"G2 Stack - Total Products Active\"\n",
        "    ]\n",
        "\n",
        "    # Identify which columns to drop that actually exist in the DataFrame\n",
        "    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "\n",
        "    if not existing_columns_to_drop:\n",
        "        print(f\"‚ö†Ô∏è None of the specified columns exist in {os.path.basename(file_path)}. Skipping column drop.\")\n",
        "        print(\"-\" * 30)\n",
        "        return\n",
        "\n",
        "    # Drop the columns\n",
        "    print(f\"‚úÖ Dropping columns: {existing_columns_to_drop}\")\n",
        "    df_updated = df.drop(columns=existing_columns_to_drop, errors='ignore')\n",
        "\n",
        "    # Save the updated DataFrame back to the same file\n",
        "    df_updated.to_csv(file_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Columns dropped. New shape: {df_updated.shape}\")\n",
        "    print(f\"üíæ Updated data saved back to {os.path.basename(file_path)}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# List of countries to process, assuming cleaned files exist\n",
        "countries_to_process = [\n",
        "    'Algeria', 'Angola', 'Armenia', 'Australia', 'Austria', 'Belarus', 'Belize',\n",
        "    'British_Indian_Ocean_Territory', 'Cameroon', 'Canada', 'Colombia', 'Congo',\n",
        "    'Cayman_Islands', 'Cyprus', 'C√¥te_dIvoire', 'Germany', 'Italy', 'Ecuador',\n",
        "    'Egypt', 'Equatorial_Guinea', 'Ethiopia', 'Faroe_Islands', 'Fiji', 'Finland',\n",
        "    'Gabon', 'Ghana', 'Greece', 'Guinea', 'Guyana', 'Honduras', 'Iceland',\n",
        "    'Indonesia', 'Iraq', 'Isle_of_Man', 'Jersey', 'Malawi', 'Marshall_Islands',\n",
        "    'Mauritania', 'Mauritius'\n",
        "]\n",
        "\n",
        "# Run the process for each country\n",
        "for country in countries_to_process:\n",
        "    drop_specified_columns(f'/content/{country}_final_filled.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1SHg827uvx_",
        "outputId": "4a85e748-fe7b-43ce-bb52-23bbe011dd96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processing Algeria_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (18, 16)\n",
            "üíæ Updated data saved back to Algeria_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Angola_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (6, 16)\n",
            "üíæ Updated data saved back to Angola_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Armenia_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (49, 16)\n",
            "üíæ Updated data saved back to Armenia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Australia_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (3004, 16)\n",
            "üíæ Updated data saved back to Australia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Austria_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (509, 16)\n",
            "üíæ Updated data saved back to Austria_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Belarus_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (38, 16)\n",
            "üíæ Updated data saved back to Belarus_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Belize_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (78, 16)\n",
            "üíæ Updated data saved back to Belize_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing British_Indian_Ocean_Territory_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (1, 16)\n",
            "üíæ Updated data saved back to British_Indian_Ocean_Territory_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cameroon_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (180, 16)\n",
            "üíæ Updated data saved back to Cameroon_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Canada_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (15369, 16)\n",
            "üíæ Updated data saved back to Canada_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Colombia_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (1962, 16)\n",
            "üíæ Updated data saved back to Colombia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Congo_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (10, 16)\n",
            "üíæ Updated data saved back to Congo_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cayman_Islands_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (103, 16)\n",
            "üíæ Updated data saved back to Cayman_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Cyprus_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (1220, 16)\n",
            "üíæ Updated data saved back to Cyprus_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing C√¥te_dIvoire_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (131, 16)\n",
            "üíæ Updated data saved back to C√¥te_dIvoire_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Germany_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (15364, 16)\n",
            "üíæ Updated data saved back to Germany_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Italy_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (8269, 16)\n",
            "üíæ Updated data saved back to Italy_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Ecuador_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (323, 16)\n",
            "üíæ Updated data saved back to Ecuador_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Egypt_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (2030, 16)\n",
            "üíæ Updated data saved back to Egypt_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Equatorial_Guinea_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (4, 16)\n",
            "üíæ Updated data saved back to Equatorial_Guinea_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Ethiopia_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (274, 16)\n",
            "üíæ Updated data saved back to Ethiopia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Faroe_Islands_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (1, 16)\n",
            "üíæ Updated data saved back to Faroe_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Fiji_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (9, 16)\n",
            "üíæ Updated data saved back to Fiji_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Finland_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (4008, 16)\n",
            "üíæ Updated data saved back to Finland_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Gabon_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (10, 16)\n",
            "üíæ Updated data saved back to Gabon_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Ghana_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (748, 16)\n",
            "üíæ Updated data saved back to Ghana_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Greece_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (1562, 16)\n",
            "üíæ Updated data saved back to Greece_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Guinea_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (16, 16)\n",
            "üíæ Updated data saved back to Guinea_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Guyana_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (18, 16)\n",
            "üíæ Updated data saved back to Guyana_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Honduras_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (56, 16)\n",
            "üíæ Updated data saved back to Honduras_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Iceland_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (446, 16)\n",
            "üíæ Updated data saved back to Iceland_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Indonesia_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (4635, 16)\n",
            "üíæ Updated data saved back to Indonesia_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Iraq_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (168, 16)\n",
            "üíæ Updated data saved back to Iraq_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Isle_of_Man_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (26, 16)\n",
            "üíæ Updated data saved back to Isle_of_Man_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Jersey_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (21, 16)\n",
            "üíæ Updated data saved back to Jersey_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Malawi_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (41, 16)\n",
            "üíæ Updated data saved back to Malawi_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Marshall_Islands_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (30, 16)\n",
            "üíæ Updated data saved back to Marshall_Islands_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Mauritania_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (4, 16)\n",
            "üíæ Updated data saved back to Mauritania_final_filled.csv\n",
            "------------------------------\n",
            "--- Processing Mauritius_final_filled.csv ---\n",
            "‚úÖ Dropping columns: ['Price']\n",
            "‚úÖ Columns dropped. New shape: (304, 16)\n",
            "üíæ Updated data saved back to Mauritius_final_filled.csv\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}